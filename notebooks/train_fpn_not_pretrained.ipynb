{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df43f32-7949-4e7a-90be-f6797c667b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting ujson\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/f8/8c/5274ba7b4df814c87a8840a58e2b1dae6a489f49c3b0fad2d15f1e41d47b/ujson-5.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45 kB)\n",
      "Installing collected packages: ujson\n",
      "Successfully installed ujson-5.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting seaborn\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/10/5b/0479d7d845b5ba410ca702ffcd7f2cd95a14a4dfff1fde2637802b258b9b/seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.20.3)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (9.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (20.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (4.33.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.11.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ujson \n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8110a508-5c22-443e-8c2d-1f2785961a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc16b26c-4d09-4f3e-9f19-dcb81493fcd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "import sys \n",
    "import json \n",
    "from src import dataset_lvis\n",
    "from src import metrics\n",
    "from src import helper_functions as helper\n",
    "import importlib\n",
    "from pathlib import Path \n",
    "from PIL import Image as PILImage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pandas as pd \n",
    "import time \n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "713acfd1-c1cf-44fa-b17f-bb640870fab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.dataset_lvis' from '/home/INM705_CW_Collins_Velagala/notebooks/../src/dataset_lvis.py'>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dataset_lvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97408296-2f70-4eaa-8243-c5fdb4345c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fa061-629f-46cb-b3df-99a9c7afbcde",
   "metadata": {},
   "source": [
    "# Load train and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39685c2e-8366-47b5-9e5b-6b40e2d8c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes : {1: 'cappuccino', 2: 'chessboard', 3: 'coffee_maker', 4: 'cowboy_hat', 5: 'drumstick', 6: 'monkey'}\n",
      "loaded 618 positive set images\n",
      "loaded 0 negative set images\n",
      "loaded 25 non-exhaustive set images\n",
      "Loaded 593 images!\n",
      "class 1 has 71 positive and 0 negative images\n",
      "class 2 has 9 positive and 0 negative images\n",
      "class 3 has 233 positive and 0 negative images\n",
      "class 4 has 199 positive and 0 negative images\n",
      "class 5 has 8 positive and 0 negative images\n",
      "class 6 has 73 positive and 0 negative images\n",
      "995 annotations found!\n",
      "stage:  train\n",
      "classes:  {'cappuccino': 206, 'chessboard': 240, 'coffee_maker': 284, 'cowboy_hat': 319, 'drumstick': 400, 'monkey': 699}\n",
      "ds_path:  ../Datasets/coco/\n",
      "labels_f:  ../Datasets/coco/annotations/lvis_v1_train.json\n",
      "imgs_dir:  ../Datasets/coco/images/train2017\n",
      "Time taken to initialize train set: 30.310505867004395\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "data_args = {'stage': 'train',\n",
    "            'classes': ['cowboy_hat', 'coffee_maker', 'monkey', 'cappuccino', 'drumstick', 'chessboard'], # ['drumstick'],#'sofa'], #, 'signboard'],\n",
    "            'ds_path' : \"../Datasets/coco/\",\n",
    "            'labels_dir': \"annotations\",\n",
    "            'images_dir': 'images',\n",
    "             'height' : 480,\n",
    "             'width' : 640,\n",
    "            'max_negative' : 0}\n",
    "train_data = dataset_lvis.LVISData(**data_args)\n",
    "\n",
    "print(f'Time taken to initialize train set: {time.time()-time_start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965951b-bfb1-48e8-8deb-c320b8d98b2f",
   "metadata": {},
   "source": [
    "### Split train set into custom train val set\n",
    "\n",
    "Validation set reserved for testing\n",
    "\n",
    "Check that we don't have too many of the rare cases in validation set - if so then there won't be much data to actually train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec426e15-a300-45cf-8d66-ee6dc4ffdd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 6, 2: 1, 3: 26, 4: 14, 5: 0, 6: 12}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what images have what classes?\n",
    "g_cpu = torch.Generator()\n",
    "g_cpu.manual_seed(2)\n",
    "\n",
    "#Split dataset to test and train\n",
    "indices = torch.randperm(len(train_data), generator=g_cpu).tolist()\n",
    "\n",
    "\n",
    "dataset_train = torch.utils.data.Subset(train_data, indices[:550])\n",
    "dataset_val = torch.utils.data.Subset(train_data, indices[550:])\n",
    "\n",
    "\n",
    "class_counts = dict(zip(range(1,7), [0]*6))\n",
    "for _, _, y in dataset_val:\n",
    "    for label in y['labels']:\n",
    "        class_counts[label.item()] += 1\n",
    "            \n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed89f8-a249-4680-bc8a-e39015c2c63d",
   "metadata": {},
   "source": [
    "## Prepare test set\n",
    "\n",
    "The LVIS validation set is reserved for our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "754ea837-6704-449b-a0b3-c5768d1b453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes : {1: 'cappuccino', 2: 'chessboard', 3: 'coffee_maker', 4: 'cowboy_hat', 5: 'drumstick', 6: 'monkey'}\n",
      "loaded 119 positive set images\n",
      "loaded 1003 negative set images\n",
      "loaded 2 non-exhaustive set images\n",
      "Loaded 1116 images!\n",
      "class 1 has 17 positive and 189 negative images\n",
      "class 2 has 1 positive and 186 negative images\n",
      "class 3 has 46 positive and 172 negative images\n",
      "class 4 has 39 positive and 188 negative images\n",
      "class 5 has 2 positive and 111 negative images\n",
      "class 6 has 12 positive and 182 negative images\n",
      "166 annotations found!\n",
      "stage:  val\n",
      "classes:  {'cappuccino': 206, 'chessboard': 240, 'coffee_maker': 284, 'cowboy_hat': 319, 'drumstick': 400, 'monkey': 699}\n",
      "ds_path:  ../Datasets/coco/\n",
      "labels_f:  ../Datasets/coco/annotations/lvis_v1_val.json\n",
      "imgs_dir:  ../Datasets/coco/images/train2017\n",
      "Time taken to initialize val set: 12.648752450942993\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time() \n",
    "data_args = {'stage': 'val',\n",
    "            'classes': ['cowboy_hat', 'coffee_maker', 'monkey', 'cappuccino', 'drumstick', 'chessboard'], #['drumstick'],#'sofa'], #, 'signboard'],\n",
    "            'ds_path' : \"../Datasets/coco/\",\n",
    "            'labels_dir': \"annotations\",\n",
    "            'images_dir': 'images',\n",
    "             'height' : 480,\n",
    "             'width' : 640,\n",
    "            'max_negative' : 200}   # note that we include negative sets for the testing\n",
    "test_data = dataset_lvis.LVISData(**data_args)\n",
    "print(f'Time taken to initialize val set: {time.time()-time_start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5210fb-ba67-46c9-a72d-2f93d89e672f",
   "metadata": {},
   "source": [
    "# Fine-tuning the model\n",
    "\n",
    "Here we set `pretrained = False` for the FPN: FPN is pre-trained on coco dataset so we'll try to retrain from scratch. We keep the backbone ResNet50 pretrained on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc402d35-ad82-4540-a29a-ed74706f7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41364461-6867-4f49-aa53-177da9f90eb2",
   "metadata": {},
   "source": [
    "# Set up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ff5ce4e-729b-49b4-a1e0-28d48fc9049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    " dataset_train, batch_size=10, shuffle=True, #num_workers=4,\n",
    " collate_fn=helper.CollateCustom())\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    " dataset_val, batch_size=10, shuffle=True, #num_workers=4,\n",
    " collate_fn=helper.CollateCustom())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af324036-b825-4573-a784-343b9dbb3a47",
   "metadata": {},
   "source": [
    "# Initialize Model + Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d139c611-3d48-4c57-8147-9fa50f2bee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has 7 classes - background and 6 specified objects\n",
    "num_classes = 7\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d532151-671e-4846-aa83-34678660c4b7",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e034c2c8-91f2-4546-84ea-b3f0085c75bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1153.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 0 epochs: 70.11650776863098\n",
      "Validation loss after 0 epochs: 4.14311671257019\n",
      "Time elapsed for 0 epochs: 90.64811968803406\n",
      "Saved checkpoint model_0_epochs.pth!\n",
      "Time elapsed after 0 epochs: 92.47265195846558\n",
      "Training loss after 1 epochs: 49.90538018941879\n",
      "Validation loss after 1 epochs: 3.9391698241233826\n",
      "Time elapsed for 1 epochs: 182.1252818107605\n",
      "Training loss after 2 epochs: 46.451831340789795\n",
      "Validation loss after 2 epochs: 3.557008147239685\n",
      "Time elapsed for 2 epochs: 271.9290690422058\n",
      "Training loss after 3 epochs: 42.485505640506744\n",
      "Validation loss after 3 epochs: 3.2987363040447235\n",
      "Time elapsed for 3 epochs: 361.43655371665955\n",
      "Training loss after 4 epochs: 39.786172449588776\n",
      "Validation loss after 4 epochs: 3.681826889514923\n",
      "Time elapsed for 4 epochs: 450.64519739151\n",
      "Training loss after 5 epochs: 37.45835664868355\n",
      "Validation loss after 5 epochs: 3.059147298336029\n",
      "Time elapsed for 5 epochs: 539.6711666584015\n",
      "Saved checkpoint model_5_epochs.pth!\n",
      "Time elapsed after 5 epochs: 541.2367963790894\n",
      "Training loss after 6 epochs: 34.3770609498024\n",
      "Validation loss after 6 epochs: 2.8061336874961853\n",
      "Time elapsed for 6 epochs: 630.4790863990784\n",
      "Training loss after 7 epochs: 31.996530145406723\n",
      "Validation loss after 7 epochs: 2.8425606787204742\n",
      "Time elapsed for 7 epochs: 719.9161627292633\n",
      "Training loss after 8 epochs: 30.75761616230011\n",
      "Validation loss after 8 epochs: 2.6447748243808746\n",
      "Time elapsed for 8 epochs: 809.6509652137756\n",
      "Training loss after 9 epochs: 28.747000336647034\n",
      "Validation loss after 9 epochs: 3.1874674260616302\n",
      "Time elapsed for 9 epochs: 899.8170902729034\n",
      "Training loss after 10 epochs: 27.113298386335373\n",
      "Validation loss after 10 epochs: 2.868620365858078\n",
      "Time elapsed for 10 epochs: 990.3393907546997\n",
      "Saved checkpoint model_10_epochs.pth!\n",
      "Time elapsed after 10 epochs: 991.8449008464813\n",
      "Training loss after 11 epochs: 25.891142040491104\n",
      "Validation loss after 11 epochs: 2.683419018983841\n",
      "Time elapsed for 11 epochs: 1082.6144070625305\n",
      "Training loss after 12 epochs: 25.593975871801376\n",
      "Validation loss after 12 epochs: 2.583999902009964\n",
      "Time elapsed for 12 epochs: 1173.3264248371124\n",
      "Training loss after 14 epochs: 22.83577471971512\n",
      "Validation loss after 14 epochs: 2.612836569547653\n",
      "Time elapsed for 14 epochs: 1355.363582611084\n",
      "Training loss after 15 epochs: 22.232726395130157\n",
      "Validation loss after 15 epochs: 2.768321603536606\n",
      "Time elapsed for 15 epochs: 1446.6437516212463\n",
      "Saved checkpoint model_15_epochs.pth!\n",
      "Time elapsed after 15 epochs: 1448.1817903518677\n",
      "Training loss after 16 epochs: 22.09590780735016\n",
      "Validation loss after 16 epochs: 3.0031979084014893\n",
      "Time elapsed for 16 epochs: 1539.7118437290192\n",
      "Training loss after 17 epochs: 19.864780128002167\n",
      "Validation loss after 17 epochs: 2.890243887901306\n",
      "Time elapsed for 17 epochs: 1631.4063062667847\n",
      "Training loss after 18 epochs: 19.812377765774727\n",
      "Validation loss after 18 epochs: 3.1180669963359833\n",
      "Time elapsed for 18 epochs: 1723.0237166881561\n",
      "Training loss after 19 epochs: 19.98783341050148\n",
      "Validation loss after 19 epochs: 2.634361654520035\n",
      "Time elapsed for 19 epochs: 1814.6735503673553\n",
      "Training loss after 20 epochs: 18.59904733300209\n",
      "Validation loss after 20 epochs: 3.103957414627075\n",
      "Time elapsed for 20 epochs: 1906.3714907169342\n",
      "Saved checkpoint model_20_epochs.pth!\n",
      "Time elapsed after 20 epochs: 1908.1832029819489\n",
      "Training loss after 21 epochs: 18.08400769531727\n",
      "Validation loss after 21 epochs: 2.7743565440177917\n",
      "Time elapsed for 21 epochs: 2000.1666803359985\n",
      "Training loss after 22 epochs: 17.977094024419785\n",
      "Validation loss after 22 epochs: 3.1464796662330627\n",
      "Time elapsed for 22 epochs: 2092.4417819976807\n",
      "Training loss after 23 epochs: 18.213057801127434\n",
      "Validation loss after 23 epochs: 3.5090726912021637\n",
      "Time elapsed for 23 epochs: 2184.581256389618\n",
      "Training loss after 24 epochs: 16.461276799440384\n",
      "Validation loss after 24 epochs: 3.6133768260478973\n",
      "Time elapsed for 24 epochs: 2276.9649062156677\n",
      "Training loss after 25 epochs: 15.551236405968666\n",
      "Validation loss after 25 epochs: 3.3102824985980988\n",
      "Time elapsed for 25 epochs: 2369.6363763809204\n",
      "Saved checkpoint model_25_epochs.pth!\n",
      "Time elapsed after 25 epochs: 2371.363953590393\n",
      "Training loss after 26 epochs: 15.84911622107029\n",
      "Validation loss after 26 epochs: 2.779125690460205\n",
      "Time elapsed for 26 epochs: 2463.956972360611\n",
      "Training loss after 27 epochs: 14.992920190095901\n",
      "Validation loss after 27 epochs: 3.2274500131607056\n",
      "Time elapsed for 27 epochs: 2556.674948692322\n",
      "Training loss after 28 epochs: 14.794517949223518\n",
      "Validation loss after 28 epochs: 2.7377650439739227\n",
      "Time elapsed for 28 epochs: 2649.4242384433746\n",
      "Training loss after 29 epochs: 14.74908596277237\n",
      "Validation loss after 29 epochs: 2.9042669534683228\n",
      "Time elapsed for 29 epochs: 2742.222689628601\n",
      "Training loss after 30 epochs: 14.556858763098717\n",
      "Validation loss after 30 epochs: 3.1132038831710815\n",
      "Time elapsed for 30 epochs: 2835.1204662323\n",
      "Saved checkpoint model_30_epochs.pth!\n",
      "Time elapsed after 30 epochs: 2836.6035027503967\n",
      "Training loss after 31 epochs: 14.388680338859558\n",
      "Validation loss after 31 epochs: 3.000867187976837\n",
      "Time elapsed for 31 epochs: 2929.4019725322723\n",
      "Training loss after 32 epochs: 13.627840921282768\n",
      "Validation loss after 32 epochs: 3.2799893617630005\n",
      "Time elapsed for 32 epochs: 3022.2900252342224\n",
      "Training loss after 33 epochs: 13.437076151371002\n",
      "Validation loss after 33 epochs: 4.290075898170471\n",
      "Time elapsed for 33 epochs: 3115.130301952362\n",
      "Training loss after 34 epochs: 13.145337134599686\n",
      "Validation loss after 34 epochs: 3.1929246485233307\n",
      "Time elapsed for 34 epochs: 3207.946197986603\n",
      "Training loss after 35 epochs: 12.816724643111229\n",
      "Validation loss after 35 epochs: 2.855393648147583\n",
      "Time elapsed for 35 epochs: 3301.115520477295\n",
      "Saved checkpoint model_35_epochs.pth!\n",
      "Time elapsed after 35 epochs: 3302.7882072925568\n",
      "Training loss after 36 epochs: 12.88546408712864\n",
      "Validation loss after 36 epochs: 3.3232009410858154\n",
      "Time elapsed for 36 epochs: 3395.8237369060516\n",
      "Training loss after 37 epochs: 12.640266850590706\n",
      "Validation loss after 37 epochs: 3.1223580837249756\n",
      "Time elapsed for 37 epochs: 3488.778332710266\n",
      "Training loss after 38 epochs: 12.424602597951889\n",
      "Validation loss after 38 epochs: 3.2635839581489563\n",
      "Time elapsed for 38 epochs: 3581.69402885437\n",
      "Training loss after 39 epochs: 12.962427332997322\n",
      "Validation loss after 39 epochs: 3.516319364309311\n",
      "Time elapsed for 39 epochs: 3674.6760914325714\n",
      "Training loss after 40 epochs: 13.036615431308746\n",
      "Validation loss after 40 epochs: 3.990502178668976\n",
      "Time elapsed for 40 epochs: 3767.5783190727234\n",
      "Saved checkpoint model_40_epochs.pth!\n",
      "Time elapsed after 40 epochs: 3769.077882528305\n",
      "Training loss after 41 epochs: 12.279062807559967\n",
      "Validation loss after 41 epochs: 3.5283992290496826\n",
      "Time elapsed for 41 epochs: 3862.07279253006\n",
      "Training loss after 42 epochs: 11.694759994745255\n",
      "Validation loss after 42 epochs: 3.6503623127937317\n",
      "Time elapsed for 42 epochs: 3955.0130879879\n",
      "Training loss after 43 epochs: 12.041883990168571\n",
      "Validation loss after 43 epochs: 3.7671881318092346\n",
      "Time elapsed for 43 epochs: 4048.0473113059998\n",
      "Training loss after 44 epochs: 12.136911042034626\n",
      "Validation loss after 44 epochs: 3.8032788038253784\n",
      "Time elapsed for 44 epochs: 4141.10648727417\n",
      "Training loss after 45 epochs: 11.543690621852875\n",
      "Validation loss after 45 epochs: 3.458674669265747\n",
      "Time elapsed for 45 epochs: 4234.231666326523\n",
      "Saved checkpoint model_45_epochs.pth!\n",
      "Time elapsed after 45 epochs: 4235.786110639572\n",
      "Training loss after 46 epochs: 11.014548569917679\n",
      "Validation loss after 46 epochs: 4.383785784244537\n",
      "Time elapsed for 46 epochs: 4328.770897865295\n",
      "Training loss after 47 epochs: 11.218621537089348\n",
      "Validation loss after 47 epochs: 3.662854492664337\n",
      "Time elapsed for 47 epochs: 4421.641701936722\n",
      "Training loss after 48 epochs: 10.934305623173714\n",
      "Validation loss after 48 epochs: 3.8791720271110535\n",
      "Time elapsed for 48 epochs: 4514.5161674022675\n",
      "Training loss after 49 epochs: 10.957147285342216\n",
      "Validation loss after 49 epochs: 3.337947338819504\n",
      "Time elapsed for 49 epochs: 4607.478649377823\n",
      "Saved model model.pth!\n",
      "Time elapsed for 50 epochs: 76.8 min\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "time_start = time.time() \n",
    "\n",
    "\n",
    "train_loss_df = pd.DataFrame(columns = ['epoch', 'loss_classifier', 'loss_box_reg', 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg', 'total_loss'])\n",
    "val_loss_df = pd.DataFrame(columns = ['epoch', 'loss_classifier', 'loss_box_reg', 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg', 'total_loss'])\n",
    "\n",
    "loss_types = ['loss_classifier', 'loss_box_reg', 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg', 'total_loss']\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = dict.fromkeys(loss_types, 0) \n",
    "    val_loss = dict.fromkeys(loss_types, 0) \n",
    "    \n",
    "    \"\"\"\n",
    "    Train \n",
    "    \"\"\"\n",
    "    for batch_num, (idx, X, y) in enumerate(train_loader):\n",
    "        #print(idx)\n",
    "        X = X.to(device)\n",
    "        y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
    "        \n",
    "        loss_dict = model(X, y) \n",
    "        \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        #save losses\n",
    "        for key in loss_types:\n",
    "            if key != 'total_loss':\n",
    "                train_loss[key] += loss_dict[key].item()\n",
    "            else: \n",
    "                train_loss['total_loss'] += losses.item()\n",
    "                \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    row = pd.DataFrame({'epoch': [epoch],\n",
    "          'loss_classifier': [train_loss['loss_classifier']/(batch_num+1)],\n",
    "          'loss_box_reg' : [train_loss['loss_box_reg']/(batch_num+1)],\n",
    "           'loss_mask': [train_loss['loss_mask']/(batch_num+1)],\n",
    "           'loss_objectness': [train_loss['loss_objectness']/(batch_num+1)],\n",
    "           'loss_rpn_box_reg': [train_loss['loss_rpn_box_reg']/(batch_num+1)],\n",
    "            'total_loss': [train_loss['total_loss']/(batch_num+1)] \n",
    "          })     \n",
    "\n",
    "    train_loss_df = pd.concat([train_loss_df, row], ignore_index = True, axis = 0)\n",
    "    \n",
    "    print(f\"Training loss after {epoch} epochs: {train_loss['total_loss']}\")\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Validation\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (idx, X, y) in enumerate(val_loader):\n",
    "            X = X.to(device)\n",
    "            y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
    "\n",
    "            loss_dict = model(X, y) \n",
    "            \n",
    "            losses_val = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            #save losses\n",
    "            for key in loss_types:\n",
    "                if key != 'total_loss':\n",
    "                    val_loss[key] += loss_dict[key].item()\n",
    "                else: \n",
    "                    val_loss['total_loss'] += losses_val.item()\n",
    "                    \n",
    "        row = pd.DataFrame({'epoch': [epoch],\n",
    "                          'loss_classifier': [val_loss['loss_classifier']/(batch_num+1)],\n",
    "                          'loss_box_reg' : [val_loss['loss_box_reg']/(batch_num+1)],\n",
    "                           'loss_mask': [val_loss['loss_mask']/(batch_num+1)],\n",
    "                           'loss_objectness': [val_loss['loss_objectness']/(batch_num+1)],\n",
    "                           'loss_rpn_box_reg': [val_loss['loss_rpn_box_reg']/(batch_num+1)],\n",
    "                            'total_loss': [val_loss['total_loss']/(batch_num+1)] \n",
    "                          })\n",
    "        val_loss_df = pd.concat([val_loss_df, row], ignore_index = True, axis = 0)\n",
    "\n",
    "    print(f\"Validation loss after {epoch} epochs: {val_loss['total_loss']}\") \n",
    "    print(f'Time elapsed for {epoch} epochs: {time.time()-time_start}') \n",
    "\n",
    "      \n",
    "    \"\"\"\n",
    "    Save checkpoints and losses every 5 epoch\n",
    "    \"\"\"\n",
    "    if epoch%5 == 0: \n",
    "        checkpoint = {\"state_dict\": model.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"step\": epoch,\n",
    "                        \"ds_args\" : data_args\n",
    "                        }\n",
    "        fname = \"model_\" + str(epoch) + \"_epochs.pth\"\n",
    "        helper.save_checkpoint(checkpoint, fname)\n",
    "        print(f'Time elapsed after {epoch} epochs: {time.time()-time_start}')  \n",
    "        val_loss_df.to_csv(Path.cwd().parent.joinpath(\"val_loss.csv\"))\n",
    "        train_loss_df.to_csv(Path.cwd().parent.joinpath(\"train_loss.csv\"))\n",
    "\n",
    "    \n",
    "    \n",
    "    #for final epoch \n",
    "    if epoch == num_epochs-1: \n",
    "        helper.save_model(model.state_dict(), \"model.pth\")\n",
    "        val_loss_df.to_csv(Path.cwd().parent.joinpath(\"val_loss.csv\"))\n",
    "        train_loss_df.to_csv(Path.cwd().parent.joinpath(\"train_loss.csv\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "print(f\"Time elapsed for {epoch+1} epochs: {round((time.time()-time_start)/60, 2)} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a9cfa-912a-4885-962c-7f7dc807ccde",
   "metadata": {},
   "source": [
    "# Plot loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96f98a05-d7e5-422a-a34e-6f2a9dece363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.274846</td>\n",
       "      <td>0.828623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.907371</td>\n",
       "      <td>0.787834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.844579</td>\n",
       "      <td>0.711402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.772464</td>\n",
       "      <td>0.659747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.723385</td>\n",
       "      <td>0.736365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.681061</td>\n",
       "      <td>0.611829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.625037</td>\n",
       "      <td>0.561227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.581755</td>\n",
       "      <td>0.568512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.559229</td>\n",
       "      <td>0.528955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.522673</td>\n",
       "      <td>0.637493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.492969</td>\n",
       "      <td>0.573724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.470748</td>\n",
       "      <td>0.536684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.465345</td>\n",
       "      <td>0.5168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.432162</td>\n",
       "      <td>0.548849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.415196</td>\n",
       "      <td>0.522567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.404231</td>\n",
       "      <td>0.553664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.401744</td>\n",
       "      <td>0.60064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.361178</td>\n",
       "      <td>0.578049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.360225</td>\n",
       "      <td>0.623613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.363415</td>\n",
       "      <td>0.526872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.338164</td>\n",
       "      <td>0.620791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.3288</td>\n",
       "      <td>0.554871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.326856</td>\n",
       "      <td>0.629296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.331147</td>\n",
       "      <td>0.701815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.299296</td>\n",
       "      <td>0.722675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.28275</td>\n",
       "      <td>0.662056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.288166</td>\n",
       "      <td>0.555825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.272599</td>\n",
       "      <td>0.64549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.268991</td>\n",
       "      <td>0.547553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.268165</td>\n",
       "      <td>0.580853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.26467</td>\n",
       "      <td>0.622641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.261612</td>\n",
       "      <td>0.600173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.247779</td>\n",
       "      <td>0.655998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.24431</td>\n",
       "      <td>0.858015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.239006</td>\n",
       "      <td>0.638585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.233031</td>\n",
       "      <td>0.571079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.234281</td>\n",
       "      <td>0.66464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.229823</td>\n",
       "      <td>0.624472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.225902</td>\n",
       "      <td>0.652717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.23568</td>\n",
       "      <td>0.703264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.237029</td>\n",
       "      <td>0.7981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.223256</td>\n",
       "      <td>0.70568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.212632</td>\n",
       "      <td>0.730072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.218943</td>\n",
       "      <td>0.753438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.220671</td>\n",
       "      <td>0.760656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.209885</td>\n",
       "      <td>0.691735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.200265</td>\n",
       "      <td>0.876757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.203975</td>\n",
       "      <td>0.732571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.198806</td>\n",
       "      <td>0.775834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.199221</td>\n",
       "      <td>0.667589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_loss  val_loss\n",
       "0    1.274846  0.828623\n",
       "1    0.907371  0.787834\n",
       "2    0.844579  0.711402\n",
       "3    0.772464  0.659747\n",
       "4    0.723385  0.736365\n",
       "5    0.681061  0.611829\n",
       "6    0.625037  0.561227\n",
       "7    0.581755  0.568512\n",
       "8    0.559229  0.528955\n",
       "9    0.522673  0.637493\n",
       "10   0.492969  0.573724\n",
       "11   0.470748  0.536684\n",
       "12   0.465345    0.5168\n",
       "13   0.432162  0.548849\n",
       "14   0.415196  0.522567\n",
       "15   0.404231  0.553664\n",
       "16   0.401744   0.60064\n",
       "17   0.361178  0.578049\n",
       "18   0.360225  0.623613\n",
       "19   0.363415  0.526872\n",
       "20   0.338164  0.620791\n",
       "21     0.3288  0.554871\n",
       "22   0.326856  0.629296\n",
       "23   0.331147  0.701815\n",
       "24   0.299296  0.722675\n",
       "25    0.28275  0.662056\n",
       "26   0.288166  0.555825\n",
       "27   0.272599   0.64549\n",
       "28   0.268991  0.547553\n",
       "29   0.268165  0.580853\n",
       "30    0.26467  0.622641\n",
       "31   0.261612  0.600173\n",
       "32   0.247779  0.655998\n",
       "33    0.24431  0.858015\n",
       "34   0.239006  0.638585\n",
       "35   0.233031  0.571079\n",
       "36   0.234281   0.66464\n",
       "37   0.229823  0.624472\n",
       "38   0.225902  0.652717\n",
       "39    0.23568  0.703264\n",
       "40   0.237029    0.7981\n",
       "41   0.223256   0.70568\n",
       "42   0.212632  0.730072\n",
       "43   0.218943  0.753438\n",
       "44   0.220671  0.760656\n",
       "45   0.209885  0.691735\n",
       "46   0.200265  0.876757\n",
       "47   0.203975  0.732571\n",
       "48   0.198806  0.775834\n",
       "49   0.199221  0.667589"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_df = pd.DataFrame() \n",
    "#new_df['epoch'] = train_loss_df['epoch'] \n",
    "new_df['train_loss'] = train_loss_df['total_loss'] \n",
    "new_df['val_loss'] = val_loss_df['total_loss'] \n",
    "\n",
    "sns.lineplot(data = new_df[1:])\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49cfaca4-1cd8-4189-bea6-512a2cf7e834",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss_classifier</th>\n",
       "      <th>loss_box_reg</th>\n",
       "      <th>loss_mask</th>\n",
       "      <th>loss_objectness</th>\n",
       "      <th>loss_rpn_box_reg</th>\n",
       "      <th>total_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.262297</td>\n",
       "      <td>0.033436</td>\n",
       "      <td>0.698337</td>\n",
       "      <td>0.241498</td>\n",
       "      <td>0.039278</td>\n",
       "      <td>1.274846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.120505</td>\n",
       "      <td>0.064663</td>\n",
       "      <td>0.578448</td>\n",
       "      <td>0.110058</td>\n",
       "      <td>0.033697</td>\n",
       "      <td>0.907371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.14373</td>\n",
       "      <td>0.094585</td>\n",
       "      <td>0.486173</td>\n",
       "      <td>0.087607</td>\n",
       "      <td>0.032483</td>\n",
       "      <td>0.844579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.140447</td>\n",
       "      <td>0.101061</td>\n",
       "      <td>0.424709</td>\n",
       "      <td>0.074406</td>\n",
       "      <td>0.031841</td>\n",
       "      <td>0.772464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.131518</td>\n",
       "      <td>0.101512</td>\n",
       "      <td>0.396109</td>\n",
       "      <td>0.065008</td>\n",
       "      <td>0.029238</td>\n",
       "      <td>0.723385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.095647</td>\n",
       "      <td>0.375343</td>\n",
       "      <td>0.062205</td>\n",
       "      <td>0.027103</td>\n",
       "      <td>0.681061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.118626</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.327996</td>\n",
       "      <td>0.048055</td>\n",
       "      <td>0.025066</td>\n",
       "      <td>0.625037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.111559</td>\n",
       "      <td>0.107615</td>\n",
       "      <td>0.301013</td>\n",
       "      <td>0.03995</td>\n",
       "      <td>0.021618</td>\n",
       "      <td>0.581755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.10826</td>\n",
       "      <td>0.110034</td>\n",
       "      <td>0.284868</td>\n",
       "      <td>0.034617</td>\n",
       "      <td>0.02145</td>\n",
       "      <td>0.559229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.102292</td>\n",
       "      <td>0.109855</td>\n",
       "      <td>0.264487</td>\n",
       "      <td>0.026617</td>\n",
       "      <td>0.019422</td>\n",
       "      <td>0.522673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.098482</td>\n",
       "      <td>0.113612</td>\n",
       "      <td>0.24073</td>\n",
       "      <td>0.021298</td>\n",
       "      <td>0.018846</td>\n",
       "      <td>0.492969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.091601</td>\n",
       "      <td>0.108445</td>\n",
       "      <td>0.233187</td>\n",
       "      <td>0.01822</td>\n",
       "      <td>0.019295</td>\n",
       "      <td>0.470748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.088265</td>\n",
       "      <td>0.104094</td>\n",
       "      <td>0.236891</td>\n",
       "      <td>0.016907</td>\n",
       "      <td>0.019187</td>\n",
       "      <td>0.465345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.07804</td>\n",
       "      <td>0.099333</td>\n",
       "      <td>0.22354</td>\n",
       "      <td>0.012474</td>\n",
       "      <td>0.018775</td>\n",
       "      <td>0.432162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.076955</td>\n",
       "      <td>0.097076</td>\n",
       "      <td>0.212442</td>\n",
       "      <td>0.011158</td>\n",
       "      <td>0.017566</td>\n",
       "      <td>0.415196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.070732</td>\n",
       "      <td>0.095616</td>\n",
       "      <td>0.21024</td>\n",
       "      <td>0.010155</td>\n",
       "      <td>0.017489</td>\n",
       "      <td>0.404231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.06994</td>\n",
       "      <td>0.096421</td>\n",
       "      <td>0.210331</td>\n",
       "      <td>0.009169</td>\n",
       "      <td>0.015882</td>\n",
       "      <td>0.401744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.060616</td>\n",
       "      <td>0.089072</td>\n",
       "      <td>0.188256</td>\n",
       "      <td>0.00749</td>\n",
       "      <td>0.015744</td>\n",
       "      <td>0.361178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.061916</td>\n",
       "      <td>0.088117</td>\n",
       "      <td>0.185684</td>\n",
       "      <td>0.007831</td>\n",
       "      <td>0.016678</td>\n",
       "      <td>0.360225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.062282</td>\n",
       "      <td>0.087303</td>\n",
       "      <td>0.188774</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.363415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.05602</td>\n",
       "      <td>0.082096</td>\n",
       "      <td>0.177502</td>\n",
       "      <td>0.007367</td>\n",
       "      <td>0.015179</td>\n",
       "      <td>0.338164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.052191</td>\n",
       "      <td>0.081438</td>\n",
       "      <td>0.174005</td>\n",
       "      <td>0.00574</td>\n",
       "      <td>0.015427</td>\n",
       "      <td>0.3288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.049946</td>\n",
       "      <td>0.080727</td>\n",
       "      <td>0.177039</td>\n",
       "      <td>0.005564</td>\n",
       "      <td>0.013579</td>\n",
       "      <td>0.326856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.050533</td>\n",
       "      <td>0.082874</td>\n",
       "      <td>0.178486</td>\n",
       "      <td>0.005081</td>\n",
       "      <td>0.014174</td>\n",
       "      <td>0.331147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.044308</td>\n",
       "      <td>0.074657</td>\n",
       "      <td>0.16224</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.013389</td>\n",
       "      <td>0.299296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.041477</td>\n",
       "      <td>0.070826</td>\n",
       "      <td>0.154336</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>0.012304</td>\n",
       "      <td>0.28275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.040987</td>\n",
       "      <td>0.072859</td>\n",
       "      <td>0.158969</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.011472</td>\n",
       "      <td>0.288166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.038819</td>\n",
       "      <td>0.067934</td>\n",
       "      <td>0.1508</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.011232</td>\n",
       "      <td>0.272599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.037544</td>\n",
       "      <td>0.068261</td>\n",
       "      <td>0.149693</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>0.010175</td>\n",
       "      <td>0.268991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.037549</td>\n",
       "      <td>0.066636</td>\n",
       "      <td>0.150472</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>0.009745</td>\n",
       "      <td>0.268165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.036996</td>\n",
       "      <td>0.067789</td>\n",
       "      <td>0.146952</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>0.009507</td>\n",
       "      <td>0.26467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.035564</td>\n",
       "      <td>0.068437</td>\n",
       "      <td>0.144747</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.009872</td>\n",
       "      <td>0.261612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.033543</td>\n",
       "      <td>0.061694</td>\n",
       "      <td>0.140471</td>\n",
       "      <td>0.003109</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>0.247779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.03329</td>\n",
       "      <td>0.059997</td>\n",
       "      <td>0.138544</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>0.009275</td>\n",
       "      <td>0.24431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.03279</td>\n",
       "      <td>0.058451</td>\n",
       "      <td>0.135473</td>\n",
       "      <td>0.00346</td>\n",
       "      <td>0.008833</td>\n",
       "      <td>0.239006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.031467</td>\n",
       "      <td>0.057337</td>\n",
       "      <td>0.133531</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.00807</td>\n",
       "      <td>0.233031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.032212</td>\n",
       "      <td>0.057332</td>\n",
       "      <td>0.131996</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>0.00988</td>\n",
       "      <td>0.234281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.031014</td>\n",
       "      <td>0.056388</td>\n",
       "      <td>0.131161</td>\n",
       "      <td>0.002905</td>\n",
       "      <td>0.008355</td>\n",
       "      <td>0.229823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0.02955</td>\n",
       "      <td>0.054835</td>\n",
       "      <td>0.130776</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.008277</td>\n",
       "      <td>0.225902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.030387</td>\n",
       "      <td>0.060751</td>\n",
       "      <td>0.134829</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>0.23568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0.030724</td>\n",
       "      <td>0.058791</td>\n",
       "      <td>0.136987</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.007988</td>\n",
       "      <td>0.237029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.028492</td>\n",
       "      <td>0.052447</td>\n",
       "      <td>0.132744</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.007319</td>\n",
       "      <td>0.223256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.027054</td>\n",
       "      <td>0.049884</td>\n",
       "      <td>0.126008</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>0.007416</td>\n",
       "      <td>0.212632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.027835</td>\n",
       "      <td>0.05402</td>\n",
       "      <td>0.127874</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.006973</td>\n",
       "      <td>0.218943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.02748</td>\n",
       "      <td>0.053364</td>\n",
       "      <td>0.129827</td>\n",
       "      <td>0.00242</td>\n",
       "      <td>0.007579</td>\n",
       "      <td>0.220671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.026738</td>\n",
       "      <td>0.050438</td>\n",
       "      <td>0.124085</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>0.00668</td>\n",
       "      <td>0.209885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.025181</td>\n",
       "      <td>0.045401</td>\n",
       "      <td>0.120958</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.00648</td>\n",
       "      <td>0.200265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.02552</td>\n",
       "      <td>0.050648</td>\n",
       "      <td>0.116944</td>\n",
       "      <td>0.002254</td>\n",
       "      <td>0.008609</td>\n",
       "      <td>0.203975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.024765</td>\n",
       "      <td>0.04761</td>\n",
       "      <td>0.118089</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.00666</td>\n",
       "      <td>0.198806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.02416</td>\n",
       "      <td>0.047473</td>\n",
       "      <td>0.11803</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.007316</td>\n",
       "      <td>0.199221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch loss_classifier loss_box_reg loss_mask loss_objectness  \\\n",
       "0      0        0.262297     0.033436  0.698337        0.241498   \n",
       "1      1        0.120505     0.064663  0.578448        0.110058   \n",
       "2      2         0.14373     0.094585  0.486173        0.087607   \n",
       "3      3        0.140447     0.101061  0.424709        0.074406   \n",
       "4      4        0.131518     0.101512  0.396109        0.065008   \n",
       "5      5        0.120763     0.095647  0.375343        0.062205   \n",
       "6      6        0.118626     0.105295  0.327996        0.048055   \n",
       "7      7        0.111559     0.107615  0.301013         0.03995   \n",
       "8      8         0.10826     0.110034  0.284868        0.034617   \n",
       "9      9        0.102292     0.109855  0.264487        0.026617   \n",
       "10    10        0.098482     0.113612   0.24073        0.021298   \n",
       "11    11        0.091601     0.108445  0.233187         0.01822   \n",
       "12    12        0.088265     0.104094  0.236891        0.016907   \n",
       "13    13         0.07804     0.099333   0.22354        0.012474   \n",
       "14    14        0.076955     0.097076  0.212442        0.011158   \n",
       "15    15        0.070732     0.095616   0.21024        0.010155   \n",
       "16    16         0.06994     0.096421  0.210331        0.009169   \n",
       "17    17        0.060616     0.089072  0.188256         0.00749   \n",
       "18    18        0.061916     0.088117  0.185684        0.007831   \n",
       "19    19        0.062282     0.087303  0.188774        0.009342   \n",
       "20    20         0.05602     0.082096  0.177502        0.007367   \n",
       "21    21        0.052191     0.081438  0.174005         0.00574   \n",
       "22    22        0.049946     0.080727  0.177039        0.005564   \n",
       "23    23        0.050533     0.082874  0.178486        0.005081   \n",
       "24    24        0.044308     0.074657   0.16224        0.004701   \n",
       "25    25        0.041477     0.070826  0.154336        0.003807   \n",
       "26    26        0.040987     0.072859  0.158969        0.003878   \n",
       "27    27        0.038819     0.067934    0.1508        0.003814   \n",
       "28    28        0.037544     0.068261  0.149693        0.003317   \n",
       "29    29        0.037549     0.066636  0.150472        0.003763   \n",
       "30    30        0.036996     0.067789  0.146952        0.003426   \n",
       "31    31        0.035564     0.068437  0.144747        0.002993   \n",
       "32    32        0.033543     0.061694  0.140471        0.003109   \n",
       "33    33         0.03329     0.059997  0.138544        0.003205   \n",
       "34    34         0.03279     0.058451  0.135473         0.00346   \n",
       "35    35        0.031467     0.057337  0.133531        0.002626   \n",
       "36    36        0.032212     0.057332  0.131996        0.002862   \n",
       "37    37        0.031014     0.056388  0.131161        0.002905   \n",
       "38    38         0.02955     0.054835  0.130776        0.002465   \n",
       "39    39        0.030387     0.060751  0.134829        0.002745   \n",
       "40    40        0.030724     0.058791  0.136987        0.002539   \n",
       "41    41        0.028492     0.052447  0.132744        0.002253   \n",
       "42    42        0.027054     0.049884  0.126008        0.002271   \n",
       "43    43        0.027835      0.05402  0.127874        0.002242   \n",
       "44    44         0.02748     0.053364  0.129827         0.00242   \n",
       "45    45        0.026738     0.050438  0.124085        0.001944   \n",
       "46    46        0.025181     0.045401  0.120958        0.002244   \n",
       "47    47         0.02552     0.050648  0.116944        0.002254   \n",
       "48    48        0.024765      0.04761  0.118089        0.001682   \n",
       "49    49         0.02416     0.047473   0.11803        0.002242   \n",
       "\n",
       "   loss_rpn_box_reg total_loss  \n",
       "0          0.039278   1.274846  \n",
       "1          0.033697   0.907371  \n",
       "2          0.032483   0.844579  \n",
       "3          0.031841   0.772464  \n",
       "4          0.029238   0.723385  \n",
       "5          0.027103   0.681061  \n",
       "6          0.025066   0.625037  \n",
       "7          0.021618   0.581755  \n",
       "8           0.02145   0.559229  \n",
       "9          0.019422   0.522673  \n",
       "10         0.018846   0.492969  \n",
       "11         0.019295   0.470748  \n",
       "12         0.019187   0.465345  \n",
       "13         0.018775   0.432162  \n",
       "14         0.017566   0.415196  \n",
       "15         0.017489   0.404231  \n",
       "16         0.015882   0.401744  \n",
       "17         0.015744   0.361178  \n",
       "18         0.016678   0.360225  \n",
       "19         0.015713   0.363415  \n",
       "20         0.015179   0.338164  \n",
       "21         0.015427     0.3288  \n",
       "22         0.013579   0.326856  \n",
       "23         0.014174   0.331147  \n",
       "24         0.013389   0.299296  \n",
       "25         0.012304    0.28275  \n",
       "26         0.011472   0.288166  \n",
       "27         0.011232   0.272599  \n",
       "28         0.010175   0.268991  \n",
       "29         0.009745   0.268165  \n",
       "30         0.009507    0.26467  \n",
       "31         0.009872   0.261612  \n",
       "32         0.008961   0.247779  \n",
       "33         0.009275    0.24431  \n",
       "34         0.008833   0.239006  \n",
       "35          0.00807   0.233031  \n",
       "36          0.00988   0.234281  \n",
       "37         0.008355   0.229823  \n",
       "38         0.008277   0.225902  \n",
       "39         0.006969    0.23568  \n",
       "40         0.007988   0.237029  \n",
       "41         0.007319   0.223256  \n",
       "42         0.007416   0.212632  \n",
       "43         0.006973   0.218943  \n",
       "44         0.007579   0.220671  \n",
       "45          0.00668   0.209885  \n",
       "46          0.00648   0.200265  \n",
       "47         0.008609   0.203975  \n",
       "48          0.00666   0.198806  \n",
       "49         0.007316   0.199221  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6116f24-8ab3-4546-9126-a23d4e476566",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss_classifier</th>\n",
       "      <th>loss_box_reg</th>\n",
       "      <th>loss_mask</th>\n",
       "      <th>loss_objectness</th>\n",
       "      <th>loss_rpn_box_reg</th>\n",
       "      <th>total_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.085024</td>\n",
       "      <td>0.039443</td>\n",
       "      <td>0.611934</td>\n",
       "      <td>0.078627</td>\n",
       "      <td>0.013595</td>\n",
       "      <td>0.828623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.136172</td>\n",
       "      <td>0.075162</td>\n",
       "      <td>0.504973</td>\n",
       "      <td>0.059921</td>\n",
       "      <td>0.011606</td>\n",
       "      <td>0.787834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.147855</td>\n",
       "      <td>0.097606</td>\n",
       "      <td>0.402613</td>\n",
       "      <td>0.053591</td>\n",
       "      <td>0.009737</td>\n",
       "      <td>0.711402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.134884</td>\n",
       "      <td>0.099948</td>\n",
       "      <td>0.360933</td>\n",
       "      <td>0.048822</td>\n",
       "      <td>0.015159</td>\n",
       "      <td>0.659747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.145978</td>\n",
       "      <td>0.095541</td>\n",
       "      <td>0.436849</td>\n",
       "      <td>0.045333</td>\n",
       "      <td>0.012665</td>\n",
       "      <td>0.736365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.122495</td>\n",
       "      <td>0.101614</td>\n",
       "      <td>0.333561</td>\n",
       "      <td>0.045135</td>\n",
       "      <td>0.009025</td>\n",
       "      <td>0.611829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.111461</td>\n",
       "      <td>0.095663</td>\n",
       "      <td>0.292545</td>\n",
       "      <td>0.051784</td>\n",
       "      <td>0.009773</td>\n",
       "      <td>0.561227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.103234</td>\n",
       "      <td>0.094178</td>\n",
       "      <td>0.324986</td>\n",
       "      <td>0.037873</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.568512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.095846</td>\n",
       "      <td>0.089874</td>\n",
       "      <td>0.296075</td>\n",
       "      <td>0.038046</td>\n",
       "      <td>0.009115</td>\n",
       "      <td>0.528955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.107208</td>\n",
       "      <td>0.08819</td>\n",
       "      <td>0.363999</td>\n",
       "      <td>0.068977</td>\n",
       "      <td>0.009119</td>\n",
       "      <td>0.637493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.106081</td>\n",
       "      <td>0.090566</td>\n",
       "      <td>0.316883</td>\n",
       "      <td>0.051618</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.573724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.083469</td>\n",
       "      <td>0.087421</td>\n",
       "      <td>0.314081</td>\n",
       "      <td>0.04423</td>\n",
       "      <td>0.007483</td>\n",
       "      <td>0.536684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.094395</td>\n",
       "      <td>0.083749</td>\n",
       "      <td>0.29415</td>\n",
       "      <td>0.037444</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.5168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.090544</td>\n",
       "      <td>0.088013</td>\n",
       "      <td>0.310411</td>\n",
       "      <td>0.052624</td>\n",
       "      <td>0.007258</td>\n",
       "      <td>0.548849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.08411</td>\n",
       "      <td>0.08102</td>\n",
       "      <td>0.291953</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>0.007584</td>\n",
       "      <td>0.522567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.090226</td>\n",
       "      <td>0.081053</td>\n",
       "      <td>0.317017</td>\n",
       "      <td>0.057798</td>\n",
       "      <td>0.00757</td>\n",
       "      <td>0.553664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.099228</td>\n",
       "      <td>0.086114</td>\n",
       "      <td>0.336025</td>\n",
       "      <td>0.070859</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>0.60064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.089755</td>\n",
       "      <td>0.075828</td>\n",
       "      <td>0.336509</td>\n",
       "      <td>0.068062</td>\n",
       "      <td>0.007895</td>\n",
       "      <td>0.578049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.08987</td>\n",
       "      <td>0.069834</td>\n",
       "      <td>0.363121</td>\n",
       "      <td>0.091842</td>\n",
       "      <td>0.008946</td>\n",
       "      <td>0.623613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.075535</td>\n",
       "      <td>0.073338</td>\n",
       "      <td>0.333832</td>\n",
       "      <td>0.036595</td>\n",
       "      <td>0.007572</td>\n",
       "      <td>0.526872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.081947</td>\n",
       "      <td>0.078598</td>\n",
       "      <td>0.40561</td>\n",
       "      <td>0.046881</td>\n",
       "      <td>0.007756</td>\n",
       "      <td>0.620791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.084489</td>\n",
       "      <td>0.063123</td>\n",
       "      <td>0.337752</td>\n",
       "      <td>0.061953</td>\n",
       "      <td>0.007554</td>\n",
       "      <td>0.554871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.084138</td>\n",
       "      <td>0.090447</td>\n",
       "      <td>0.372911</td>\n",
       "      <td>0.073785</td>\n",
       "      <td>0.008016</td>\n",
       "      <td>0.629296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.091961</td>\n",
       "      <td>0.071821</td>\n",
       "      <td>0.322048</td>\n",
       "      <td>0.201235</td>\n",
       "      <td>0.014748</td>\n",
       "      <td>0.701815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.110757</td>\n",
       "      <td>0.073732</td>\n",
       "      <td>0.345325</td>\n",
       "      <td>0.178517</td>\n",
       "      <td>0.014345</td>\n",
       "      <td>0.722675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.111201</td>\n",
       "      <td>0.07183</td>\n",
       "      <td>0.375879</td>\n",
       "      <td>0.093083</td>\n",
       "      <td>0.010063</td>\n",
       "      <td>0.662056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.088047</td>\n",
       "      <td>0.069096</td>\n",
       "      <td>0.300766</td>\n",
       "      <td>0.088398</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.555825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.10057</td>\n",
       "      <td>0.071313</td>\n",
       "      <td>0.352934</td>\n",
       "      <td>0.110489</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>0.64549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.095089</td>\n",
       "      <td>0.078032</td>\n",
       "      <td>0.281539</td>\n",
       "      <td>0.084929</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>0.547553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.10223</td>\n",
       "      <td>0.073774</td>\n",
       "      <td>0.30324</td>\n",
       "      <td>0.093328</td>\n",
       "      <td>0.008283</td>\n",
       "      <td>0.580853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.109203</td>\n",
       "      <td>0.074624</td>\n",
       "      <td>0.335196</td>\n",
       "      <td>0.095009</td>\n",
       "      <td>0.008609</td>\n",
       "      <td>0.622641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.094114</td>\n",
       "      <td>0.071857</td>\n",
       "      <td>0.341793</td>\n",
       "      <td>0.084193</td>\n",
       "      <td>0.008216</td>\n",
       "      <td>0.600173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.095664</td>\n",
       "      <td>0.069207</td>\n",
       "      <td>0.38288</td>\n",
       "      <td>0.099152</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>0.655998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.124701</td>\n",
       "      <td>0.068633</td>\n",
       "      <td>0.426545</td>\n",
       "      <td>0.221933</td>\n",
       "      <td>0.016202</td>\n",
       "      <td>0.858015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.112014</td>\n",
       "      <td>0.075729</td>\n",
       "      <td>0.346371</td>\n",
       "      <td>0.096718</td>\n",
       "      <td>0.007754</td>\n",
       "      <td>0.638585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.091283</td>\n",
       "      <td>0.069761</td>\n",
       "      <td>0.310857</td>\n",
       "      <td>0.090869</td>\n",
       "      <td>0.008308</td>\n",
       "      <td>0.571079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.113732</td>\n",
       "      <td>0.080654</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.115819</td>\n",
       "      <td>0.008281</td>\n",
       "      <td>0.66464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.121024</td>\n",
       "      <td>0.071237</td>\n",
       "      <td>0.31578</td>\n",
       "      <td>0.106886</td>\n",
       "      <td>0.009545</td>\n",
       "      <td>0.624472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0.115863</td>\n",
       "      <td>0.080954</td>\n",
       "      <td>0.314681</td>\n",
       "      <td>0.130719</td>\n",
       "      <td>0.010499</td>\n",
       "      <td>0.652717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.120935</td>\n",
       "      <td>0.074017</td>\n",
       "      <td>0.367056</td>\n",
       "      <td>0.131419</td>\n",
       "      <td>0.009837</td>\n",
       "      <td>0.703264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0.159106</td>\n",
       "      <td>0.090691</td>\n",
       "      <td>0.415121</td>\n",
       "      <td>0.123601</td>\n",
       "      <td>0.009581</td>\n",
       "      <td>0.7981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.123295</td>\n",
       "      <td>0.074756</td>\n",
       "      <td>0.377402</td>\n",
       "      <td>0.120287</td>\n",
       "      <td>0.00994</td>\n",
       "      <td>0.70568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.129845</td>\n",
       "      <td>0.069622</td>\n",
       "      <td>0.388522</td>\n",
       "      <td>0.131831</td>\n",
       "      <td>0.010252</td>\n",
       "      <td>0.730072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.146875</td>\n",
       "      <td>0.072029</td>\n",
       "      <td>0.406106</td>\n",
       "      <td>0.119771</td>\n",
       "      <td>0.008656</td>\n",
       "      <td>0.753438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.125873</td>\n",
       "      <td>0.078838</td>\n",
       "      <td>0.421163</td>\n",
       "      <td>0.12577</td>\n",
       "      <td>0.009012</td>\n",
       "      <td>0.760656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.127856</td>\n",
       "      <td>0.076776</td>\n",
       "      <td>0.352583</td>\n",
       "      <td>0.125438</td>\n",
       "      <td>0.009082</td>\n",
       "      <td>0.691735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.147052</td>\n",
       "      <td>0.068004</td>\n",
       "      <td>0.510501</td>\n",
       "      <td>0.14043</td>\n",
       "      <td>0.010769</td>\n",
       "      <td>0.876757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.138952</td>\n",
       "      <td>0.077903</td>\n",
       "      <td>0.373262</td>\n",
       "      <td>0.132789</td>\n",
       "      <td>0.009666</td>\n",
       "      <td>0.732571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.128409</td>\n",
       "      <td>0.073697</td>\n",
       "      <td>0.427883</td>\n",
       "      <td>0.136112</td>\n",
       "      <td>0.009733</td>\n",
       "      <td>0.775834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.118431</td>\n",
       "      <td>0.068658</td>\n",
       "      <td>0.348673</td>\n",
       "      <td>0.122905</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>0.667589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch loss_classifier loss_box_reg loss_mask loss_objectness  \\\n",
       "0      0        0.085024     0.039443  0.611934        0.078627   \n",
       "1      1        0.136172     0.075162  0.504973        0.059921   \n",
       "2      2        0.147855     0.097606  0.402613        0.053591   \n",
       "3      3        0.134884     0.099948  0.360933        0.048822   \n",
       "4      4        0.145978     0.095541  0.436849        0.045333   \n",
       "5      5        0.122495     0.101614  0.333561        0.045135   \n",
       "6      6        0.111461     0.095663  0.292545        0.051784   \n",
       "7      7        0.103234     0.094178  0.324986        0.037873   \n",
       "8      8        0.095846     0.089874  0.296075        0.038046   \n",
       "9      9        0.107208      0.08819  0.363999        0.068977   \n",
       "10    10        0.106081     0.090566  0.316883        0.051618   \n",
       "11    11        0.083469     0.087421  0.314081         0.04423   \n",
       "12    12        0.094395     0.083749   0.29415        0.037444   \n",
       "13    13        0.090544     0.088013  0.310411        0.052624   \n",
       "14    14         0.08411      0.08102  0.291953          0.0579   \n",
       "15    15        0.090226     0.081053  0.317017        0.057798   \n",
       "16    16        0.099228     0.086114  0.336025        0.070859   \n",
       "17    17        0.089755     0.075828  0.336509        0.068062   \n",
       "18    18         0.08987     0.069834  0.363121        0.091842   \n",
       "19    19        0.075535     0.073338  0.333832        0.036595   \n",
       "20    20        0.081947     0.078598   0.40561        0.046881   \n",
       "21    21        0.084489     0.063123  0.337752        0.061953   \n",
       "22    22        0.084138     0.090447  0.372911        0.073785   \n",
       "23    23        0.091961     0.071821  0.322048        0.201235   \n",
       "24    24        0.110757     0.073732  0.345325        0.178517   \n",
       "25    25        0.111201      0.07183  0.375879        0.093083   \n",
       "26    26        0.088047     0.069096  0.300766        0.088398   \n",
       "27    27         0.10057     0.071313  0.352934        0.110489   \n",
       "28    28        0.095089     0.078032  0.281539        0.084929   \n",
       "29    29         0.10223     0.073774   0.30324        0.093328   \n",
       "30    30        0.109203     0.074624  0.335196        0.095009   \n",
       "31    31        0.094114     0.071857  0.341793        0.084193   \n",
       "32    32        0.095664     0.069207   0.38288        0.099152   \n",
       "33    33        0.124701     0.068633  0.426545        0.221933   \n",
       "34    34        0.112014     0.075729  0.346371        0.096718   \n",
       "35    35        0.091283     0.069761  0.310857        0.090869   \n",
       "36    36        0.113732     0.080654  0.346154        0.115819   \n",
       "37    37        0.121024     0.071237   0.31578        0.106886   \n",
       "38    38        0.115863     0.080954  0.314681        0.130719   \n",
       "39    39        0.120935     0.074017  0.367056        0.131419   \n",
       "40    40        0.159106     0.090691  0.415121        0.123601   \n",
       "41    41        0.123295     0.074756  0.377402        0.120287   \n",
       "42    42        0.129845     0.069622  0.388522        0.131831   \n",
       "43    43        0.146875     0.072029  0.406106        0.119771   \n",
       "44    44        0.125873     0.078838  0.421163         0.12577   \n",
       "45    45        0.127856     0.076776  0.352583        0.125438   \n",
       "46    46        0.147052     0.068004  0.510501         0.14043   \n",
       "47    47        0.138952     0.077903  0.373262        0.132789   \n",
       "48    48        0.128409     0.073697  0.427883        0.136112   \n",
       "49    49        0.118431     0.068658  0.348673        0.122905   \n",
       "\n",
       "   loss_rpn_box_reg total_loss  \n",
       "0          0.013595   0.828623  \n",
       "1          0.011606   0.787834  \n",
       "2          0.009737   0.711402  \n",
       "3          0.015159   0.659747  \n",
       "4          0.012665   0.736365  \n",
       "5          0.009025   0.611829  \n",
       "6          0.009773   0.561227  \n",
       "7          0.008242   0.568512  \n",
       "8          0.009115   0.528955  \n",
       "9          0.009119   0.637493  \n",
       "10         0.008575   0.573724  \n",
       "11         0.007483   0.536684  \n",
       "12         0.007063     0.5168  \n",
       "13         0.007258   0.548849  \n",
       "14         0.007584   0.522567  \n",
       "15          0.00757   0.553664  \n",
       "16         0.008413    0.60064  \n",
       "17         0.007895   0.578049  \n",
       "18         0.008946   0.623613  \n",
       "19         0.007572   0.526872  \n",
       "20         0.007756   0.620791  \n",
       "21         0.007554   0.554871  \n",
       "22         0.008016   0.629296  \n",
       "23         0.014748   0.701815  \n",
       "24         0.014345   0.722675  \n",
       "25         0.010063   0.662056  \n",
       "26         0.009518   0.555825  \n",
       "27         0.010185    0.64549  \n",
       "28         0.007963   0.547553  \n",
       "29         0.008283   0.580853  \n",
       "30         0.008609   0.622641  \n",
       "31         0.008216   0.600173  \n",
       "32         0.009094   0.655998  \n",
       "33         0.016202   0.858015  \n",
       "34         0.007754   0.638585  \n",
       "35         0.008308   0.571079  \n",
       "36         0.008281    0.66464  \n",
       "37         0.009545   0.624472  \n",
       "38         0.010499   0.652717  \n",
       "39         0.009837   0.703264  \n",
       "40         0.009581     0.7981  \n",
       "41          0.00994    0.70568  \n",
       "42         0.010252   0.730072  \n",
       "43         0.008656   0.753438  \n",
       "44         0.009012   0.760656  \n",
       "45         0.009082   0.691735  \n",
       "46         0.010769   0.876757  \n",
       "47         0.009666   0.732571  \n",
       "48         0.009733   0.775834  \n",
       "49         0.008923   0.667589  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e4a1c-eeec-454d-a4e1-e309680683d2",
   "metadata": {},
   "source": [
    "## Run the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "618c3906-f0c3-4bcf-88ba-6f4bdff67749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.dataset_lvis' from '/home/INM705_CW_Collins_Velagala/notebooks/../src/dataset_lvis.py'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(metrics)\n",
    "importlib.reload(dataset_lvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abe51460-a7b3-495c-9832-e7c1be7fec8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    " test_data, batch_size=1, shuffle=True, #num_workers=4,\n",
    " collate_fn=helper.CollateCustom())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e492a-4b1f-483c-bdba-04d5557f4eca",
   "metadata": {},
   "source": [
    "If testing at different time from training, then load checkpoints.\n",
    "\n",
    "Or load checkpoints from earlier stopping point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50934e2d-0ee8-4b7c-aa8a-2a49117941d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint ../model_checkpoints/model_10_epochs.pth!\n"
     ]
    }
   ],
   "source": [
    "cp_path = Path('../model_checkpoints/model_10_epochs.pth')\n",
    "helper.load_checkpoint(cp_path, model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c5b251e-30e4-43eb-b80e-24d4b132627e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 52s, sys: 8min 5s, total: 11min 58s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pred_boxes = []\n",
    "gt = []\n",
    "for idx, X, y in test_loader:\n",
    "    model.eval()\n",
    "    y_pred = model(X.to(device))\n",
    "    # keep running list of predictions and ground truths\n",
    "    pred_boxes, gt = metrics.store_preds(idx, y, y_pred,\n",
    "                                     pred_boxes, gt)\n",
    "\n",
    "mAP_list = []\n",
    "thresholds = [0.5, 0.7, 0.8, 0.9, 0.95]\n",
    "for iou_thresh in thresholds:\n",
    "    mAP_list.append(metrics.calculate_ap(pred_boxes, gt, \n",
    "                               iou_threshold=iou_thresh,\n",
    "                               class_datasets = test_data.class_datasets))\n",
    "\n",
    "print('\\n\\n\\n--------------\\n\\n')\n",
    "for i in range(len(thresholds)):\n",
    "    print('mAP for iou_threshold of {}: {:.3f}'.format(thresholds[i], mAP_list[i][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a3124-be12-4a3f-bcb5-4c0f23a0f93b",
   "metadata": {},
   "source": [
    "### Print out per category mAP\n",
    "\n",
    "Table below shows the categories we have in rows, and different IoU thresholds in columns.\n",
    "\n",
    "We also print the number of images in the test set containing each category. You can see that the model doesn't have much chance to prove itself on chessboard or drumstick categories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a37a1e9d-16d8-4f6a-ba6e-877cca106e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positives</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "      <th>0.95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cappuccino</th>\n",
       "      <td>17</td>\n",
       "      <td>0.2398</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chessboard</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coffee_maker</th>\n",
       "      <td>46</td>\n",
       "      <td>0.6546</td>\n",
       "      <td>0.4761</td>\n",
       "      <td>0.1520</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cowboy_hat</th>\n",
       "      <td>39</td>\n",
       "      <td>0.3491</td>\n",
       "      <td>0.1301</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drumstick</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monkey</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              positives     0.5     0.7     0.8     0.9    0.95\n",
       "cappuccino           17  0.2398  0.0889  0.0476  0.0000  0.0000\n",
       "chessboard            1  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "coffee_maker         46  0.6546  0.4761  0.1520  0.0041  0.0004\n",
       "cowboy_hat           39  0.3491  0.1301  0.0094  0.0001  0.0000\n",
       "drumstick             2  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "monkey               12  0.0073  0.0000  0.0000  0.0000  0.0000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(index = list(range(1, 7)), columns = ['positives'] + thresholds)\n",
    "\n",
    "df['positives'] = [len(c['positive']) for c in test_data.class_datasets.values()]\n",
    "\n",
    "for i in range(len(thresholds)):\n",
    "    data = [v.item() for v in mAP_list[i][1].values()]\n",
    "    df[thresholds[i]] = data\n",
    "\n",
    "df.rename(index=test_data.idx_to_classname, inplace=True)\n",
    "df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0dbfe7-dd5e-404a-b142-a4eaf974bb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
