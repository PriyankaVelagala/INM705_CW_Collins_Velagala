{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df43f32-7949-4e7a-90be-f6797c667b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting ujson\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/f8/8c/5274ba7b4df814c87a8840a58e2b1dae6a489f49c3b0fad2d15f1e41d47b/ujson-5.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45 kB)\n",
      "Installing collected packages: ujson\n",
      "Successfully installed ujson-5.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting seaborn\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/10/5b/0479d7d845b5ba410ca702ffcd7f2cd95a14a4dfff1fde2637802b258b9b/seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.6.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (20.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (9.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.11.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ujson \n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8110a508-5c22-443e-8c2d-1f2785961a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc16b26c-4d09-4f3e-9f19-dcb81493fcd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "import sys \n",
    "import json \n",
    "from src import dataset_lvis\n",
    "from src import metrics\n",
    "from src import helper_functions as helper\n",
    "import importlib\n",
    "from pathlib import Path \n",
    "from PIL import Image as PILImage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pandas as pd \n",
    "import time \n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "713acfd1-c1cf-44fa-b17f-bb640870fab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.dataset_lvis' from '/home/INM705/INM705_CW_Collins_Velagala/notebooks/../src/dataset_lvis.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dataset_lvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97408296-2f70-4eaa-8243-c5fdb4345c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fa061-629f-46cb-b3df-99a9c7afbcde",
   "metadata": {},
   "source": [
    "# Load train and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39685c2e-8366-47b5-9e5b-6b40e2d8c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes : {1: 'cappuccino', 2: 'chessboard', 3: 'coffee_maker', 4: 'cowboy_hat', 5: 'drumstick', 6: 'monkey'}\n",
      "loaded 618 positive set images\n",
      "loaded 898 negative set images\n",
      "loaded 25 non-exhaustive set images\n",
      "Loaded 1489 images!\n",
      "class 1 has 71 positive and 150 negative images\n",
      "class 2 has 9 positive and 150 negative images\n",
      "class 3 has 233 positive and 150 negative images\n",
      "class 4 has 199 positive and 150 negative images\n",
      "class 5 has 8 positive and 150 negative images\n",
      "class 6 has 73 positive and 150 negative images\n",
      "995 annotations found!\n",
      "stage:  train\n",
      "classes:  {'cappuccino': 206, 'chessboard': 240, 'coffee_maker': 284, 'cowboy_hat': 319, 'drumstick': 400, 'monkey': 699}\n",
      "ds_path:  /home/Datasets/coco/\n",
      "labels_f:  /home/Datasets/coco/annotations/lvis_v1_train.json\n",
      "imgs_dir:  /home/Datasets/coco/images/train2017\n",
      "Time taken to initialize train set: 39.41743063926697\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "data_args = {'stage': 'train',\n",
    "            'classes': ['cowboy_hat', 'coffee_maker', 'monkey', 'cappuccino', 'drumstick', 'chessboard'], # ['drumstick'],#'sofa'], #, 'signboard'],\n",
    "            'ds_path' : \"/home/Datasets/coco/\",\n",
    "            'labels_dir': \"annotations\",\n",
    "            'images_dir': 'images',\n",
    "             'height' : 480,\n",
    "             'width' : 640,\n",
    "            'max_negative' : 150}\n",
    "train_data = dataset_lvis.LVISData(**data_args)\n",
    "\n",
    "print(f'Time taken to initialize train set: {time.time()-time_start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "754ea837-6704-449b-a0b3-c5768d1b453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes : {1: 'cappuccino', 2: 'chessboard', 3: 'coffee_maker', 4: 'cowboy_hat', 5: 'drumstick', 6: 'monkey'}\n",
      "loaded 119 positive set images\n",
      "loaded 839 negative set images\n",
      "loaded 2 non-exhaustive set images\n",
      "Loaded 953 images!\n",
      "class 1 has 17 positive and 150 negative images\n",
      "class 2 has 1 positive and 150 negative images\n",
      "class 3 has 46 positive and 150 negative images\n",
      "class 4 has 39 positive and 150 negative images\n",
      "class 5 has 2 positive and 111 negative images\n",
      "class 6 has 12 positive and 150 negative images\n",
      "166 annotations found!\n",
      "stage:  val\n",
      "classes:  {'cappuccino': 206, 'chessboard': 240, 'coffee_maker': 284, 'cowboy_hat': 319, 'drumstick': 400, 'monkey': 699}\n",
      "ds_path:  /home/Datasets/coco/\n",
      "labels_f:  /home/Datasets/coco/annotations/lvis_v1_val.json\n",
      "imgs_dir:  /home/Datasets/coco/images/train2017\n",
      "Time taken to initialize val set: 3.600599527359009\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time() \n",
    "data_args = {'stage': 'val',\n",
    "            'classes': ['cowboy_hat', 'coffee_maker', 'monkey', 'cappuccino', 'drumstick', 'chessboard'], #['drumstick'],#'sofa'], #, 'signboard'],\n",
    "            'ds_path' : \"/home/Datasets/coco/\",\n",
    "            'labels_dir': \"annotations\",\n",
    "            'images_dir': 'images',\n",
    "             'height' : 480,\n",
    "             'width' : 640,\n",
    "            'max_negative' : 150}\n",
    "val_data = dataset_lvis.LVISData(**data_args)\n",
    "print(f'Time taken to initialize val set: {time.time()-time_start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5210fb-ba67-46c9-a72d-2f93d89e672f",
   "metadata": {},
   "source": [
    "# Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc402d35-ad82-4540-a29a-ed74706f7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41364461-6867-4f49-aa53-177da9f90eb2",
   "metadata": {},
   "source": [
    "# Set up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ff5ce4e-729b-49b4-a1e0-28d48fc9049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    " train_data, batch_size=5, shuffle=True, #num_workers=4,\n",
    " collate_fn=helper.CollateCustom())\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    " val_data, batch_size=5, shuffle=True, #num_workers=4,\n",
    " collate_fn=helper.CollateCustom())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af324036-b825-4573-a784-343b9dbb3a47",
   "metadata": {},
   "source": [
    "# Initialize Model + Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d139c611-3d48-4c57-8147-9fa50f2bee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 7\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d532151-671e-4846-aa83-34678660c4b7",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034c2c8-91f2-4546-84ea-b3f0085c75bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train it for 10 epochs\n",
    "\n",
    "num_epochs = 2\n",
    "time_start = time.time() \n",
    "\n",
    "\n",
    "train_loss_df = pd.DataFrame(columns = ['epoch', 'loss_classifier', 'loss_box_reg', 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg', 'total_loss'])\n",
    "val_loss_df = pd.DataFrame(columns = ['epoch', 'loss_classifier', 'loss_box_reg', 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg', 'total_loss'])\n",
    "\n",
    "loss_types = ['loss_classifier', 'loss_box_reg', 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg', 'total_loss']\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = dict.fromkeys(loss_types, 0) \n",
    "    val_loss = dict.fromkeys(loss_types, 0) \n",
    "    \n",
    "    \"\"\"\n",
    "    Train \n",
    "    \"\"\"\n",
    "    for batch_num, (idx, X, y) in enumerate(train_loader):\n",
    "        #print(idx)\n",
    "        X = X.to(device)\n",
    "        y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
    "        \n",
    "        loss_dict = model(X, y) \n",
    "        \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        #save losses\n",
    "        for key in loss_types:\n",
    "            if key != 'total_loss':\n",
    "                train_loss[key] += loss_dict[key].item()\n",
    "            else: \n",
    "                train_loss['total_loss'] = losses.item()\n",
    "                \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    row = pd.DataFrame({'epoch': [epoch],\n",
    "          'loss_classifier': [train_loss['loss_classifier']/(batch_num+1)],\n",
    "          'loss_box_reg' : [train_loss['loss_box_reg']/(batch_num+1)],\n",
    "           'loss_mask': [train_loss['loss_mask']/(batch_num+1)],\n",
    "           'loss_objectness': [train_loss['loss_objectness']/(batch_num+1)],\n",
    "           'loss_rpn_box_reg': [train_loss['loss_rpn_box_reg']/(batch_num+1)],\n",
    "            'total_loss': [train_loss['total_loss']/(batch_num+1)] \n",
    "          })     \n",
    "\n",
    "    train_loss_df = pd.concat([train_loss_df, row], ignore_index = True, axis = 0)\n",
    "    \n",
    "    print(f\"Training loss after {epoch} epochs: {train_loss['total_loss']}\")\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Validation\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (idx, X, y) in enumerate(val_loader):\n",
    "            X = X.to(device)\n",
    "            y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
    "\n",
    "            loss_dict = model(X, y) \n",
    "            \n",
    "            losses_val = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            #save losses\n",
    "            for key in loss_types:\n",
    "                if key != 'total_loss':\n",
    "                    val_loss[key] += loss_dict[key].item()\n",
    "                else: \n",
    "                    val_loss['total_loss'] = losses_val.item()\n",
    "                    \n",
    "        row = pd.DataFrame({'epoch': [epoch],\n",
    "                          'loss_classifier': [val_loss['loss_classifier']/(batch_num+1)],\n",
    "                          'loss_box_reg' : [val_loss['loss_box_reg']/(batch_num+1)],\n",
    "                           'loss_mask': [val_loss['loss_mask']/(batch_num+1)],\n",
    "                           'loss_objectness': [val_loss['loss_objectness']/(batch_num+1)],\n",
    "                           'loss_rpn_box_reg': [val_loss['loss_rpn_box_reg']/(batch_num+1)],\n",
    "                            'total_loss': [val_loss['total_loss']/(batch_num+1)] \n",
    "                          })\n",
    "        val_loss_df = pd.concat([val_loss_df, row], ignore_index = True, axis = 0)\n",
    "\n",
    "    print(f\"Validation loss after {epoch} epochs: {val_loss['total_loss']}\") \n",
    "    print(f'Time elapsed for {epoch} epochs: {time.time()-time_start}') \n",
    "\n",
    "      \n",
    "    \"\"\"\n",
    "    Save checkpoints and losses every 5 epoch\n",
    "    \"\"\"\n",
    "    if epoch%5 == 0: \n",
    "        checkpoint = {\"state_dict\": model.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"step\": epoch,\n",
    "                        \"ds_args\" : data_args\n",
    "                        }\n",
    "        fname = \"model_\" + str(epoch) + \"_epochs.pth\"\n",
    "        helper.save_checkpoint(checkpoint, fname)\n",
    "        print(f'Time elapsed after {epoch} epochs: {time.time()-time_start}')  \n",
    "        val_loss_df.to_csv(Path.cwd().parent.joinpath(\"val_loss.csv\"))\n",
    "        train_loss_df.to_csv(Path.cwd().parent.joinpath(\"train_loss.csv\"))\n",
    "\n",
    "    \n",
    "    \n",
    "    #for final epoch \n",
    "    if epoch == num_epochs-1: \n",
    "        helper.save_model(model.state_dict(), \"model.pth\")\n",
    "        val_loss_df.to_csv(Path.cwd().parent.joinpath(\"val_loss.csv\"))\n",
    "        train_loss_df.to_csv(Path.cwd().parent.joinpath(\"train_loss.csv\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "print(f\"Time elapsed for {epoch+1} epochs: {round((time.time()-time_start)/60, 2)} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a9cfa-912a-4885-962c-7f7dc807ccde",
   "metadata": {},
   "source": [
    "# Plot loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96f98a05-d7e5-422a-a34e-6f2a9dece363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.000161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  train_loss  val_loss\n",
       "0   0.001885  0.000161\n",
       "1   0.000522  0.000101"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_df = pd.DataFrame() \n",
    "#new_df['epoch'] = train_loss_df['epoch'] \n",
    "new_df['train_loss'] = train_loss_df['total_loss'] \n",
    "new_df['val_loss'] = val_loss_df['total_loss'] \n",
    "\n",
    "sns.lineplot(data = new_df[1:])\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49cfaca4-1cd8-4189-bea6-512a2cf7e834",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-75cb848784c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"total_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sns.lineplot(data=loss_df, x=\"epoch\", y=\"total_loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6116f24-8ab3-4546-9126-a23d4e476566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
