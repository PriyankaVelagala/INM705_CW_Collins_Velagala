{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df43f32-7949-4e7a-90be-f6797c667b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting ujson\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/f8/8c/5274ba7b4df814c87a8840a58e2b1dae6a489f49c3b0fad2d15f1e41d47b/ujson-5.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45 kB)\n",
      "Installing collected packages: ujson\n",
      "Successfully installed ujson-5.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting seaborn\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/10/5b/0479d7d845b5ba410ca702ffcd7f2cd95a14a4dfff1fde2637802b258b9b/seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.6.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (20.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (9.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.11.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ujson \n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8110a508-5c22-443e-8c2d-1f2785961a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc16b26c-4d09-4f3e-9f19-dcb81493fcd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "import sys \n",
    "import json \n",
    "from src import dataset_lvis\n",
    "from src import metrics\n",
    "from src import helper_functions as helper\n",
    "import importlib\n",
    "from pathlib import Path \n",
    "from PIL import Image as PILImage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pandas as pd \n",
    "import time \n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "713acfd1-c1cf-44fa-b17f-bb640870fab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.dataset_lvis' from '/home/INM705/INM705_CW_Collins_Velagala/notebooks/../src/dataset_lvis.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dataset_lvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97408296-2f70-4eaa-8243-c5fdb4345c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fa061-629f-46cb-b3df-99a9c7afbcde",
   "metadata": {},
   "source": [
    "# Load train and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39685c2e-8366-47b5-9e5b-6b40e2d8c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes : {1: 'cappuccino', 2: 'chessboard', 3: 'coffee_maker', 4: 'cowboy_hat', 5: 'drumstick', 6: 'monkey'}\n",
      "loaded 618 positive set images\n",
      "loaded 898 negative set images\n",
      "loaded 25 non-exhaustive set images\n",
      "Loaded 1489 images!\n",
      "class 1 has 71 positive and 150 negative images\n",
      "class 2 has 9 positive and 150 negative images\n",
      "class 3 has 233 positive and 150 negative images\n",
      "class 4 has 199 positive and 150 negative images\n",
      "class 5 has 8 positive and 150 negative images\n",
      "class 6 has 73 positive and 150 negative images\n",
      "995 annotations found!\n",
      "stage:  train\n",
      "classes:  {'cappuccino': 206, 'chessboard': 240, 'coffee_maker': 284, 'cowboy_hat': 319, 'drumstick': 400, 'monkey': 699}\n",
      "ds_path:  /home/Datasets/coco/\n",
      "labels_f:  /home/Datasets/coco/annotations/lvis_v1_train.json\n",
      "imgs_dir:  /home/Datasets/coco/images/train2017\n",
      "Time taken to initialize train set: 41.61019277572632\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "data_args = {'stage': 'train',\n",
    "            'classes': ['cowboy_hat', 'coffee_maker', 'monkey', 'cappuccino', 'drumstick', 'chessboard'], # ['drumstick'],#'sofa'], #, 'signboard'],\n",
    "            'ds_path' : \"/home/Datasets/coco/\",\n",
    "            'labels_dir': \"annotations\",\n",
    "            'images_dir': 'images',\n",
    "             'height' : 480,\n",
    "             'width' : 640,\n",
    "            'max_negative' : 150}\n",
    "train_data = dataset_lvis.LVISData(**data_args)\n",
    "\n",
    "print(f'Time taken to initialize train set: {time.time()-time_start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "754ea837-6704-449b-a0b3-c5768d1b453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes : {1: 'cappuccino', 2: 'chessboard', 3: 'coffee_maker', 4: 'cowboy_hat', 5: 'drumstick', 6: 'monkey'}\n",
      "loaded 119 positive set images\n",
      "loaded 839 negative set images\n",
      "loaded 2 non-exhaustive set images\n",
      "Loaded 953 images!\n",
      "class 1 has 17 positive and 150 negative images\n",
      "class 2 has 1 positive and 150 negative images\n",
      "class 3 has 46 positive and 150 negative images\n",
      "class 4 has 39 positive and 150 negative images\n",
      "class 5 has 2 positive and 111 negative images\n",
      "class 6 has 12 positive and 150 negative images\n",
      "166 annotations found!\n",
      "stage:  val\n",
      "classes:  {'cappuccino': 206, 'chessboard': 240, 'coffee_maker': 284, 'cowboy_hat': 319, 'drumstick': 400, 'monkey': 699}\n",
      "ds_path:  /home/Datasets/coco/\n",
      "labels_f:  /home/Datasets/coco/annotations/lvis_v1_val.json\n",
      "imgs_dir:  /home/Datasets/coco/images/train2017\n",
      "Time taken to initialize val set: 3.6433255672454834\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time() \n",
    "data_args = {'stage': 'val',\n",
    "            'classes': ['cowboy_hat', 'coffee_maker', 'monkey', 'cappuccino', 'drumstick', 'chessboard'], #['drumstick'],#'sofa'], #, 'signboard'],\n",
    "            'ds_path' : \"/home/Datasets/coco/\",\n",
    "            'labels_dir': \"annotations\",\n",
    "            'images_dir': 'images',\n",
    "             'height' : 480,\n",
    "             'width' : 640,\n",
    "            'max_negative' : 150}\n",
    "val_data = dataset_lvis.LVISData(**data_args)\n",
    "print(f'Time taken to initialize val set: {time.time()-time_start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5210fb-ba67-46c9-a72d-2f93d89e672f",
   "metadata": {},
   "source": [
    "# Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc402d35-ad82-4540-a29a-ed74706f7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41364461-6867-4f49-aa53-177da9f90eb2",
   "metadata": {},
   "source": [
    "# Set up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ff5ce4e-729b-49b4-a1e0-28d48fc9049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    " train_data, batch_size=10, shuffle=True, #num_workers=4,\n",
    " collate_fn=helper.CollateCustom())\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    " val_data, batch_size=10, shuffle=True, #num_workers=4,\n",
    " collate_fn=helper.CollateCustom())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af324036-b825-4573-a784-343b9dbb3a47",
   "metadata": {},
   "source": [
    "# Initialize Model + Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d139c611-3d48-4c57-8147-9fa50f2bee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 7\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d532151-671e-4846-aa83-34678660c4b7",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e034c2c8-91f2-4546-84ea-b3f0085c75bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 0 epochs: 55.24518710374832\n",
      "Validation loss after 0 epochs: 7.716893404722214\n",
      "Time elapsed for 0 epochs: 97.67824578285217\n",
      "Saved checkpoint model_0_epochs.pth!\n",
      "Time elapsed after 0 epochs: 98.79443454742432\n",
      "Training loss after 1 epochs: 35.87267026305199\n",
      "Validation loss after 1 epochs: 7.1240220963954926\n",
      "Time elapsed for 1 epochs: 198.05672240257263\n",
      "Saved model model.pth!\n",
      "Time elapsed for 2 epochs: 3.31 min\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "time_start = time.time() \n",
    "\n",
    "\n",
    "train_loss_df = pd.DataFrame(columns = ['epoch', 'loss_classifier', 'loss_box_reg', 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg', 'total_loss'])\n",
    "val_loss_df = pd.DataFrame(columns = ['epoch', 'loss_classifier', 'loss_box_reg', 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg', 'total_loss'])\n",
    "\n",
    "loss_types = ['loss_classifier', 'loss_box_reg', 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg', 'total_loss']\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = dict.fromkeys(loss_types, 0) \n",
    "    val_loss = dict.fromkeys(loss_types, 0) \n",
    "    \n",
    "    \"\"\"\n",
    "    Train \n",
    "    \"\"\"\n",
    "    for batch_num, (idx, X, y) in enumerate(train_loader):\n",
    "        #print(idx)\n",
    "        X = X.to(device)\n",
    "        y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
    "        \n",
    "        loss_dict = model(X, y) \n",
    "        \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        #save losses\n",
    "        for key in loss_types:\n",
    "            if key != 'total_loss':\n",
    "                train_loss[key] += loss_dict[key].item()\n",
    "            else: \n",
    "                train_loss['total_loss'] += losses.item()\n",
    "                \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    row = pd.DataFrame({'epoch': [epoch],\n",
    "          'loss_classifier': [train_loss['loss_classifier']/(batch_num+1)],\n",
    "          'loss_box_reg' : [train_loss['loss_box_reg']/(batch_num+1)],\n",
    "           'loss_mask': [train_loss['loss_mask']/(batch_num+1)],\n",
    "           'loss_objectness': [train_loss['loss_objectness']/(batch_num+1)],\n",
    "           'loss_rpn_box_reg': [train_loss['loss_rpn_box_reg']/(batch_num+1)],\n",
    "            'total_loss': [train_loss['total_loss']/(batch_num+1)] \n",
    "          })     \n",
    "\n",
    "    train_loss_df = pd.concat([train_loss_df, row], ignore_index = True, axis = 0)\n",
    "    \n",
    "    print(f\"Training loss after {epoch} epochs: {train_loss['total_loss']}\")\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Validation\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (idx, X, y) in enumerate(val_loader):\n",
    "            X = X.to(device)\n",
    "            y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
    "\n",
    "            loss_dict = model(X, y) \n",
    "            \n",
    "            losses_val = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            #save losses\n",
    "            for key in loss_types:\n",
    "                if key != 'total_loss':\n",
    "                    val_loss[key] += loss_dict[key].item()\n",
    "                else: \n",
    "                    val_loss['total_loss'] += losses_val.item()\n",
    "                    \n",
    "        row = pd.DataFrame({'epoch': [epoch],\n",
    "                          'loss_classifier': [val_loss['loss_classifier']/(batch_num+1)],\n",
    "                          'loss_box_reg' : [val_loss['loss_box_reg']/(batch_num+1)],\n",
    "                           'loss_mask': [val_loss['loss_mask']/(batch_num+1)],\n",
    "                           'loss_objectness': [val_loss['loss_objectness']/(batch_num+1)],\n",
    "                           'loss_rpn_box_reg': [val_loss['loss_rpn_box_reg']/(batch_num+1)],\n",
    "                            'total_loss': [val_loss['total_loss']/(batch_num+1)] \n",
    "                          })\n",
    "        val_loss_df = pd.concat([val_loss_df, row], ignore_index = True, axis = 0)\n",
    "\n",
    "    print(f\"Validation loss after {epoch} epochs: {val_loss['total_loss']}\") \n",
    "    print(f'Time elapsed for {epoch} epochs: {time.time()-time_start}') \n",
    "\n",
    "      \n",
    "    \"\"\"\n",
    "    Save checkpoints and losses every 5 epoch\n",
    "    \"\"\"\n",
    "    if epoch%5 == 0: \n",
    "        checkpoint = {\"state_dict\": model.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"step\": epoch,\n",
    "                        \"ds_args\" : data_args\n",
    "                        }\n",
    "        fname = \"model_\" + str(epoch) + \"_epochs.pth\"\n",
    "        helper.save_checkpoint(checkpoint, fname)\n",
    "        print(f'Time elapsed after {epoch} epochs: {time.time()-time_start}')  \n",
    "        val_loss_df.to_csv(Path.cwd().parent.joinpath(\"val_loss.csv\"))\n",
    "        train_loss_df.to_csv(Path.cwd().parent.joinpath(\"train_loss.csv\"))\n",
    "\n",
    "    \n",
    "    \n",
    "    #for final epoch \n",
    "    if epoch == num_epochs-1: \n",
    "        helper.save_model(model.state_dict(), \"model.pth\")\n",
    "        val_loss_df.to_csv(Path.cwd().parent.joinpath(\"val_loss.csv\"))\n",
    "        train_loss_df.to_csv(Path.cwd().parent.joinpath(\"train_loss.csv\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "print(f\"Time elapsed for {epoch+1} epochs: {round((time.time()-time_start)/60, 2)} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a9cfa-912a-4885-962c-7f7dc807ccde",
   "metadata": {},
   "source": [
    "# Plot loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96f98a05-d7e5-422a-a34e-6f2a9dece363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.920753</td>\n",
       "      <td>0.593607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.597878</td>\n",
       "      <td>0.548002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  train_loss  val_loss\n",
       "0   0.920753  0.593607\n",
       "1   0.597878  0.548002"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_df = pd.DataFrame() \n",
    "#new_df['epoch'] = train_loss_df['epoch'] \n",
    "new_df['train_loss'] = train_loss_df['total_loss'] \n",
    "new_df['val_loss'] = val_loss_df['total_loss'] \n",
    "\n",
    "sns.lineplot(data = new_df[1:])\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49cfaca4-1cd8-4189-bea6-512a2cf7e834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss_classifier</th>\n",
       "      <th>loss_box_reg</th>\n",
       "      <th>loss_mask</th>\n",
       "      <th>loss_objectness</th>\n",
       "      <th>loss_rpn_box_reg</th>\n",
       "      <th>total_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.240526</td>\n",
       "      <td>0.123334</td>\n",
       "      <td>0.46842</td>\n",
       "      <td>0.063658</td>\n",
       "      <td>0.024815</td>\n",
       "      <td>0.920753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.145721</td>\n",
       "      <td>0.151916</td>\n",
       "      <td>0.262939</td>\n",
       "      <td>0.022273</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.597878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  epoch loss_classifier loss_box_reg loss_mask loss_objectness  \\\n",
       "0     0        0.240526     0.123334   0.46842        0.063658   \n",
       "1     1        0.145721     0.151916  0.262939        0.022273   \n",
       "\n",
       "  loss_rpn_box_reg total_loss  \n",
       "0         0.024815   0.920753  \n",
       "1         0.015029   0.597878  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6116f24-8ab3-4546-9126-a23d4e476566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss_classifier</th>\n",
       "      <th>loss_box_reg</th>\n",
       "      <th>loss_mask</th>\n",
       "      <th>loss_objectness</th>\n",
       "      <th>loss_rpn_box_reg</th>\n",
       "      <th>total_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.132092</td>\n",
       "      <td>0.117159</td>\n",
       "      <td>0.283429</td>\n",
       "      <td>0.03464</td>\n",
       "      <td>0.026286</td>\n",
       "      <td>0.593607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116492</td>\n",
       "      <td>0.136724</td>\n",
       "      <td>0.247314</td>\n",
       "      <td>0.030745</td>\n",
       "      <td>0.016726</td>\n",
       "      <td>0.548002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  epoch loss_classifier loss_box_reg loss_mask loss_objectness  \\\n",
       "0     0        0.132092     0.117159  0.283429         0.03464   \n",
       "1     1        0.116492     0.136724  0.247314        0.030745   \n",
       "\n",
       "  loss_rpn_box_reg total_loss  \n",
       "0         0.026286   0.593607  \n",
       "1         0.016726   0.548002  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e55ff-9490-4366-a193-11fff2eb5133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
