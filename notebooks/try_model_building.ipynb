{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df43f32-7949-4e7a-90be-f6797c667b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting ujson\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/f8/8c/5274ba7b4df814c87a8840a58e2b1dae6a489f49c3b0fad2d15f1e41d47b/ujson-5.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45 kB)\n",
      "Installing collected packages: ujson\n",
      "Successfully installed ujson-5.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting seaborn\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/10/5b/0479d7d845b5ba410ca702ffcd7f2cd95a14a4dfff1fde2637802b258b9b/seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.6.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (20.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (9.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.11.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ujson \n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8110a508-5c22-443e-8c2d-1f2785961a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc16b26c-4d09-4f3e-9f19-dcb81493fcd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "import sys \n",
    "import json \n",
    "from src import dataset_lvis\n",
    "from src import metrics\n",
    "from src import helper_functions as helper\n",
    "import importlib\n",
    "from pathlib import Path \n",
    "from PIL import Image as PILImage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pandas as pd \n",
    "import time \n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "713acfd1-c1cf-44fa-b17f-bb640870fab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.dataset_lvis' from '/home/INM705/INM705_CW_Collins_Velagala/notebooks/../src/dataset_lvis.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dataset_lvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97408296-2f70-4eaa-8243-c5fdb4345c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fa061-629f-46cb-b3df-99a9c7afbcde",
   "metadata": {},
   "source": [
    "# Load train and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "754ea837-6704-449b-a0b3-c5768d1b453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes : {1: 'cappuccino', 2: 'chessboard', 3: 'coffee_maker', 4: 'cowboy_hat', 5: 'drumstick', 6: 'monkey'}\n",
      "loaded 119 positive set images\n",
      "loaded 839 negative set images\n",
      "loaded 2 non-exhaustive set images\n",
      "Loaded 953 images!\n",
      "class 1 has 17 positive and 150 negative images\n",
      "class 2 has 1 positive and 150 negative images\n",
      "class 3 has 46 positive and 150 negative images\n",
      "class 4 has 39 positive and 150 negative images\n",
      "class 5 has 2 positive and 111 negative images\n",
      "class 6 has 12 positive and 150 negative images\n",
      "166 annotations found!\n",
      "stage:  val\n",
      "classes:  {'cappuccino': 206, 'chessboard': 240, 'coffee_maker': 284, 'cowboy_hat': 319, 'drumstick': 400, 'monkey': 699}\n",
      "ds_path:  /home/Datasets/coco/\n",
      "labels_f:  /home/Datasets/coco/annotations/lvis_v1_val.json\n",
      "imgs_dir:  /home/Datasets/coco/images/train2017\n",
      "Time taken to initialize val set: 3.6433255672454834\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time() \n",
    "data_args = {'stage': 'val',\n",
    "            'classes': ['cowboy_hat', 'coffee_maker', 'monkey', 'cappuccino', 'drumstick', 'chessboard'], #['drumstick'],#'sofa'], #, 'signboard'],\n",
    "            'ds_path' : \"/home/Datasets/coco/\",\n",
    "            'labels_dir': \"annotations\",\n",
    "            'images_dir': 'images',\n",
    "             'height' : 480,\n",
    "             'width' : 640,\n",
    "            'max_negative' : 150}\n",
    "val_data = dataset_lvis.LVISData(**data_args)\n",
    "print(f'Time taken to initialize val set: {time.time()-time_start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39685c2e-8366-47b5-9e5b-6b40e2d8c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes : {1: 'cappuccino', 2: 'chessboard', 3: 'coffee_maker', 4: 'cowboy_hat', 5: 'drumstick', 6: 'monkey'}\n",
      "loaded 618 positive set images\n",
      "loaded 898 negative set images\n",
      "loaded 25 non-exhaustive set images\n",
      "Loaded 1489 images!\n",
      "class 1 has 71 positive and 150 negative images\n",
      "class 2 has 9 positive and 150 negative images\n",
      "class 3 has 233 positive and 150 negative images\n",
      "class 4 has 199 positive and 150 negative images\n",
      "class 5 has 8 positive and 150 negative images\n",
      "class 6 has 73 positive and 150 negative images\n",
      "995 annotations found!\n",
      "stage:  train\n",
      "classes:  {'cappuccino': 206, 'chessboard': 240, 'coffee_maker': 284, 'cowboy_hat': 319, 'drumstick': 400, 'monkey': 699}\n",
      "ds_path:  /home/Datasets/coco/\n",
      "labels_f:  /home/Datasets/coco/annotations/lvis_v1_train.json\n",
      "imgs_dir:  /home/Datasets/coco/images/train2017\n",
      "Time taken to initialize train set: 41.61019277572632\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "data_args = {'stage': 'train',\n",
    "            'classes': ['cowboy_hat', 'coffee_maker', 'monkey', 'cappuccino', 'drumstick', 'chessboard'], # ['drumstick'],#'sofa'], #, 'signboard'],\n",
    "            'ds_path' : \"/home/Datasets/coco/\",\n",
    "            'labels_dir': \"annotations\",\n",
    "            'images_dir': 'images',\n",
    "             'height' : 480,\n",
    "             'width' : 640,\n",
    "            'max_negative' : 150}\n",
    "train_data = dataset_lvis.LVISData(**data_args)\n",
    "\n",
    "print(f'Time taken to initialize train set: {time.time()-time_start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5210fb-ba67-46c9-a72d-2f93d89e672f",
   "metadata": {},
   "source": [
    "# Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc402d35-ad82-4540-a29a-ed74706f7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41364461-6867-4f49-aa53-177da9f90eb2",
   "metadata": {},
   "source": [
    "# Set up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ff5ce4e-729b-49b4-a1e0-28d48fc9049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    " train_data, batch_size=10, shuffle=True, #num_workers=4,\n",
    " collate_fn=helper.CollateCustom())\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    " val_data, batch_size=10, shuffle=True, #num_workers=4,\n",
    " collate_fn=helper.CollateCustom())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af324036-b825-4573-a784-343b9dbb3a47",
   "metadata": {},
   "source": [
    "# Initialize Model + Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d139c611-3d48-4c57-8147-9fa50f2bee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 7\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d532151-671e-4846-aa83-34678660c4b7",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e034c2c8-91f2-4546-84ea-b3f0085c75bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 0 epochs: 88.95443008840084\n",
      "Validation loss after 0 epochs: 25.819587755948305\n",
      "Time elapsed for 0 epochs: 285.8734335899353\n",
      "Saved checkpoint model_0_epochs.pth!\n",
      "Time elapsed after 0 epochs: 287.43318939208984\n",
      "Training loss after 1 epochs: 56.28547774255276\n",
      "Validation loss after 1 epochs: 22.69692256115377\n",
      "Time elapsed for 1 epochs: 575.1260716915131\n",
      "Training loss after 2 epochs: 43.75883714854717\n",
      "Validation loss after 2 epochs: 22.394775161519647\n",
      "Time elapsed for 2 epochs: 864.1753265857697\n",
      "Training loss after 3 epochs: 36.82448152452707\n",
      "Validation loss after 3 epochs: 20.83810950256884\n",
      "Time elapsed for 3 epochs: 1153.4227318763733\n",
      "Training loss after 4 epochs: 31.54233399219811\n",
      "Validation loss after 4 epochs: 21.46841900795698\n",
      "Time elapsed for 4 epochs: 1443.0499744415283\n",
      "Training loss after 5 epochs: 29.398412494920194\n",
      "Validation loss after 5 epochs: 20.944890710990876\n",
      "Time elapsed for 5 epochs: 1732.859902381897\n",
      "Saved checkpoint model_5_epochs.pth!\n",
      "Time elapsed after 5 epochs: 1734.0765318870544\n",
      "Training loss after 6 epochs: 28.334967248141766\n",
      "Validation loss after 6 epochs: 21.83594924909994\n",
      "Time elapsed for 6 epochs: 2022.950711965561\n",
      "Training loss after 7 epochs: 27.55842760205269\n",
      "Validation loss after 7 epochs: 23.99969983426854\n",
      "Time elapsed for 7 epochs: 2312.9808106422424\n",
      "Training loss after 8 epochs: 24.67939279228449\n",
      "Validation loss after 8 epochs: 23.88205855898559\n",
      "Time elapsed for 8 epochs: 2602.301938533783\n",
      "Training loss after 9 epochs: 23.26148818945512\n",
      "Validation loss after 9 epochs: 21.938293039798737\n",
      "Time elapsed for 9 epochs: 2892.3720173835754\n",
      "Training loss after 10 epochs: 21.28029588679783\n",
      "Validation loss after 10 epochs: 20.981437472859398\n",
      "Time elapsed for 10 epochs: 3183.111401081085\n",
      "Saved checkpoint model_10_epochs.pth!\n",
      "Time elapsed after 10 epochs: 3183.644341468811\n",
      "Training loss after 11 epochs: 21.545000615529716\n",
      "Validation loss after 11 epochs: 22.5511734043248\n",
      "Time elapsed for 11 epochs: 3474.122343301773\n",
      "Training loss after 12 epochs: 20.77236718311906\n",
      "Validation loss after 12 epochs: 21.747936420608312\n",
      "Time elapsed for 12 epochs: 3763.3927109241486\n",
      "Training loss after 13 epochs: 20.424011029303074\n",
      "Validation loss after 13 epochs: 22.962133152177557\n",
      "Time elapsed for 13 epochs: 4052.812242269516\n",
      "Training loss after 14 epochs: 21.15987454354763\n",
      "Validation loss after 14 epochs: 24.43400478363037\n",
      "Time elapsed for 14 epochs: 4342.115872144699\n",
      "Training loss after 15 epochs: 19.423961653839797\n",
      "Validation loss after 15 epochs: 26.86360343499109\n",
      "Time elapsed for 15 epochs: 4632.677650690079\n",
      "Saved checkpoint model_15_epochs.pth!\n",
      "Time elapsed after 15 epochs: 4633.246307611465\n",
      "Training loss after 16 epochs: 19.100463321665302\n",
      "Validation loss after 16 epochs: 27.611401149071753\n",
      "Time elapsed for 16 epochs: 4922.144671916962\n",
      "Training loss after 17 epochs: 18.50408112630248\n",
      "Validation loss after 17 epochs: 24.203164776787162\n",
      "Time elapsed for 17 epochs: 5211.399394989014\n",
      "Training loss after 18 epochs: 18.468817511224188\n",
      "Validation loss after 18 epochs: 24.540469203493558\n",
      "Time elapsed for 18 epochs: 5502.224869012833\n",
      "Training loss after 19 epochs: 18.26749101933092\n",
      "Validation loss after 19 epochs: 25.553317887475714\n",
      "Time elapsed for 19 epochs: 5794.818466901779\n",
      "Training loss after 20 epochs: 17.589847531402484\n",
      "Validation loss after 20 epochs: 23.022950016893446\n",
      "Time elapsed for 20 epochs: 6084.3480405807495\n",
      "Saved checkpoint model_20_epochs.pth!\n",
      "Time elapsed after 20 epochs: 6084.881073474884\n",
      "Training loss after 21 epochs: 17.925774624571204\n",
      "Validation loss after 21 epochs: 28.349408489186317\n",
      "Time elapsed for 21 epochs: 6374.577149629593\n",
      "Training loss after 22 epochs: 18.18620295659639\n",
      "Validation loss after 22 epochs: 27.65294708753936\n",
      "Time elapsed for 22 epochs: 6664.482733011246\n",
      "Training loss after 23 epochs: 17.29831176251173\n",
      "Validation loss after 23 epochs: 24.296301381662488\n",
      "Time elapsed for 23 epochs: 6954.118194580078\n",
      "Training loss after 24 epochs: 17.21966470289044\n",
      "Validation loss after 24 epochs: 26.633485820144415\n",
      "Time elapsed for 24 epochs: 7244.432051420212\n",
      "Training loss after 25 epochs: 17.203777031973004\n",
      "Validation loss after 25 epochs: 26.621647918829694\n",
      "Time elapsed for 25 epochs: 7534.838881731033\n",
      "Saved checkpoint model_25_epochs.pth!\n",
      "Time elapsed after 25 epochs: 7535.3826813697815\n",
      "Training loss after 26 epochs: 16.90390519052744\n",
      "Validation loss after 26 epochs: 24.639756672317162\n",
      "Time elapsed for 26 epochs: 7825.056607246399\n",
      "Training loss after 27 epochs: 16.63584234425798\n",
      "Validation loss after 27 epochs: 25.76139516150579\n",
      "Time elapsed for 27 epochs: 8114.43477845192\n",
      "Training loss after 28 epochs: 17.37858259200584\n",
      "Validation loss after 28 epochs: 32.15128839260433\n",
      "Time elapsed for 28 epochs: 8403.96716427803\n",
      "Training loss after 29 epochs: 17.044890824705362\n",
      "Validation loss after 29 epochs: 25.18476315913722\n",
      "Time elapsed for 29 epochs: 8694.057723522186\n",
      "Training loss after 30 epochs: 16.791307844687253\n",
      "Validation loss after 30 epochs: 31.27899036742747\n",
      "Time elapsed for 30 epochs: 8984.429360628128\n",
      "Saved checkpoint model_30_epochs.pth!\n",
      "Time elapsed after 30 epochs: 8984.988461256027\n",
      "Training loss after 31 epochs: 16.29868376813829\n",
      "Validation loss after 31 epochs: 24.703284970950335\n",
      "Time elapsed for 31 epochs: 9275.556625127792\n",
      "Training loss after 32 epochs: 16.371249662712216\n",
      "Validation loss after 32 epochs: 28.388031812384725\n",
      "Time elapsed for 32 epochs: 9565.836591482162\n",
      "Training loss after 33 epochs: 16.454929798841476\n",
      "Validation loss after 33 epochs: 26.021926527493633\n",
      "Time elapsed for 33 epochs: 9858.02178144455\n",
      "Training loss after 34 epochs: 15.891321819275618\n",
      "Validation loss after 34 epochs: 25.420139023452066\n",
      "Time elapsed for 34 epochs: 10147.908422470093\n",
      "Training loss after 35 epochs: 15.934881199151278\n",
      "Validation loss after 35 epochs: 29.99696640332695\n",
      "Time elapsed for 35 epochs: 10437.306013584137\n",
      "Saved checkpoint model_35_epochs.pth!\n",
      "Time elapsed after 35 epochs: 10437.848040103912\n",
      "Training loss after 36 epochs: 16.105781588703394\n",
      "Validation loss after 36 epochs: 23.87782992387656\n",
      "Time elapsed for 36 epochs: 10727.526003837585\n",
      "Training loss after 37 epochs: 19.828600134933367\n",
      "Validation loss after 37 epochs: 30.654286927659996\n",
      "Time elapsed for 37 epochs: 11017.647716760635\n",
      "Training loss after 38 epochs: 18.193960999138653\n",
      "Validation loss after 38 epochs: 26.424109103390947\n",
      "Time elapsed for 38 epochs: 11308.587943315506\n",
      "Training loss after 39 epochs: 17.089006742462516\n",
      "Validation loss after 39 epochs: 25.166240285150707\n",
      "Time elapsed for 39 epochs: 11598.515212774277\n",
      "Training loss after 40 epochs: 16.579675087705255\n",
      "Validation loss after 40 epochs: 30.44664116052445\n",
      "Time elapsed for 40 epochs: 11888.726917743683\n",
      "Saved checkpoint model_40_epochs.pth!\n",
      "Time elapsed after 40 epochs: 11889.289184093475\n",
      "Training loss after 41 epochs: 15.617948789033107\n",
      "Validation loss after 41 epochs: 30.714927402033936\n",
      "Time elapsed for 41 epochs: 12179.395337820053\n",
      "Training loss after 42 epochs: 15.9068936817348\n",
      "Validation loss after 42 epochs: 29.37254636688158\n",
      "Time elapsed for 42 epochs: 12469.401731014252\n",
      "Training loss after 43 epochs: 15.829206440597773\n",
      "Validation loss after 43 epochs: 29.20521208306309\n",
      "Time elapsed for 43 epochs: 12779.222286939621\n",
      "Training loss after 44 epochs: 15.271492507890798\n",
      "Validation loss after 44 epochs: 30.44773637963226\n",
      "Time elapsed for 44 epochs: 13092.462264537811\n",
      "Training loss after 45 epochs: 15.313010329031385\n",
      "Validation loss after 45 epochs: 29.47917695948854\n",
      "Time elapsed for 45 epochs: 13404.641411066055\n",
      "Saved checkpoint model_45_epochs.pth!\n",
      "Time elapsed after 45 epochs: 13405.195437908173\n",
      "Training loss after 46 epochs: 15.143111610319465\n",
      "Validation loss after 46 epochs: 31.70327256829478\n",
      "Time elapsed for 46 epochs: 13715.863559007645\n",
      "Training loss after 47 epochs: 15.56934746261686\n",
      "Validation loss after 47 epochs: 26.77975474926643\n",
      "Time elapsed for 47 epochs: 14030.176964521408\n",
      "Training loss after 48 epochs: 15.464194878935814\n",
      "Validation loss after 48 epochs: 26.574781802250072\n",
      "Time elapsed for 48 epochs: 14320.624968528748\n",
      "Training loss after 49 epochs: 15.271724809426814\n",
      "Validation loss after 49 epochs: 32.588500240584835\n",
      "Time elapsed for 49 epochs: 14610.672828674316\n",
      "Saved model model.pth!\n",
      "Time elapsed for 50 epochs: 243.52 min\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "time_start = time.time() \n",
    "\n",
    "\n",
    "train_loss_df = pd.DataFrame(columns = ['epoch', 'loss_classifier', 'loss_box_reg', 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg', 'total_loss'])\n",
    "val_loss_df = pd.DataFrame(columns = ['epoch', 'loss_classifier', 'loss_box_reg', 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg', 'total_loss'])\n",
    "\n",
    "loss_types = ['loss_classifier', 'loss_box_reg', 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg', 'total_loss']\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = dict.fromkeys(loss_types, 0) \n",
    "    val_loss = dict.fromkeys(loss_types, 0) \n",
    "    \n",
    "    \"\"\"\n",
    "    Train \n",
    "    \"\"\"\n",
    "    for batch_num, (idx, X, y) in enumerate(train_loader):\n",
    "        #print(idx)\n",
    "        X = X.to(device)\n",
    "        y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
    "        \n",
    "        loss_dict = model(X, y) \n",
    "        \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        #save losses\n",
    "        for key in loss_types:\n",
    "            if key != 'total_loss':\n",
    "                train_loss[key] += loss_dict[key].item()\n",
    "            else: \n",
    "                train_loss['total_loss'] += losses.item()\n",
    "                \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    row = pd.DataFrame({'epoch': [epoch],\n",
    "          'loss_classifier': [train_loss['loss_classifier']/(batch_num+1)],\n",
    "          'loss_box_reg' : [train_loss['loss_box_reg']/(batch_num+1)],\n",
    "           'loss_mask': [train_loss['loss_mask']/(batch_num+1)],\n",
    "           'loss_objectness': [train_loss['loss_objectness']/(batch_num+1)],\n",
    "           'loss_rpn_box_reg': [train_loss['loss_rpn_box_reg']/(batch_num+1)],\n",
    "            'total_loss': [train_loss['total_loss']/(batch_num+1)] \n",
    "          })     \n",
    "\n",
    "    train_loss_df = pd.concat([train_loss_df, row], ignore_index = True, axis = 0)\n",
    "    \n",
    "    print(f\"Training loss after {epoch} epochs: {train_loss['total_loss']}\")\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Validation\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (idx, X, y) in enumerate(val_loader):\n",
    "            X = X.to(device)\n",
    "            y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
    "\n",
    "            loss_dict = model(X, y) \n",
    "            \n",
    "            losses_val = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            #save losses\n",
    "            for key in loss_types:\n",
    "                if key != 'total_loss':\n",
    "                    val_loss[key] += loss_dict[key].item()\n",
    "                else: \n",
    "                    val_loss['total_loss'] += losses_val.item()\n",
    "                    \n",
    "        row = pd.DataFrame({'epoch': [epoch],\n",
    "                          'loss_classifier': [val_loss['loss_classifier']/(batch_num+1)],\n",
    "                          'loss_box_reg' : [val_loss['loss_box_reg']/(batch_num+1)],\n",
    "                           'loss_mask': [val_loss['loss_mask']/(batch_num+1)],\n",
    "                           'loss_objectness': [val_loss['loss_objectness']/(batch_num+1)],\n",
    "                           'loss_rpn_box_reg': [val_loss['loss_rpn_box_reg']/(batch_num+1)],\n",
    "                            'total_loss': [val_loss['total_loss']/(batch_num+1)] \n",
    "                          })\n",
    "        val_loss_df = pd.concat([val_loss_df, row], ignore_index = True, axis = 0)\n",
    "\n",
    "    print(f\"Validation loss after {epoch} epochs: {val_loss['total_loss']}\") \n",
    "    print(f'Time elapsed for {epoch} epochs: {time.time()-time_start}') \n",
    "\n",
    "      \n",
    "    \"\"\"\n",
    "    Save checkpoints and losses every 5 epoch\n",
    "    \"\"\"\n",
    "    if epoch%5 == 0: \n",
    "        checkpoint = {\"state_dict\": model.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"step\": epoch,\n",
    "                        \"ds_args\" : data_args\n",
    "                        }\n",
    "        fname = \"model_\" + str(epoch) + \"_epochs.pth\"\n",
    "        helper.save_checkpoint(checkpoint, fname)\n",
    "        print(f'Time elapsed after {epoch} epochs: {time.time()-time_start}')  \n",
    "        val_loss_df.to_csv(Path.cwd().parent.joinpath(\"val_loss.csv\"))\n",
    "        train_loss_df.to_csv(Path.cwd().parent.joinpath(\"train_loss.csv\"))\n",
    "\n",
    "    \n",
    "    \n",
    "    #for final epoch \n",
    "    if epoch == num_epochs-1: \n",
    "        helper.save_model(model.state_dict(), \"model.pth\")\n",
    "        val_loss_df.to_csv(Path.cwd().parent.joinpath(\"val_loss.csv\"))\n",
    "        train_loss_df.to_csv(Path.cwd().parent.joinpath(\"train_loss.csv\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "print(f\"Time elapsed for {epoch+1} epochs: {round((time.time()-time_start)/60, 2)} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a9cfa-912a-4885-962c-7f7dc807ccde",
   "metadata": {},
   "source": [
    "# Plot loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96f98a05-d7e5-422a-a34e-6f2a9dece363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.59701</td>\n",
       "      <td>0.268954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.377755</td>\n",
       "      <td>0.236426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.293683</td>\n",
       "      <td>0.233279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.247144</td>\n",
       "      <td>0.217064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.211694</td>\n",
       "      <td>0.223629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.197305</td>\n",
       "      <td>0.218176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.190168</td>\n",
       "      <td>0.227458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.184956</td>\n",
       "      <td>0.249997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.165634</td>\n",
       "      <td>0.248771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.156117</td>\n",
       "      <td>0.228524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.142821</td>\n",
       "      <td>0.218557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.144597</td>\n",
       "      <td>0.234908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.139412</td>\n",
       "      <td>0.226541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.137074</td>\n",
       "      <td>0.239189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.142013</td>\n",
       "      <td>0.254521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.130362</td>\n",
       "      <td>0.279829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.128191</td>\n",
       "      <td>0.287619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.124188</td>\n",
       "      <td>0.252116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.123952</td>\n",
       "      <td>0.25563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.122601</td>\n",
       "      <td>0.26618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.118053</td>\n",
       "      <td>0.239822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.120307</td>\n",
       "      <td>0.295306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.122055</td>\n",
       "      <td>0.288052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.116096</td>\n",
       "      <td>0.253086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.115568</td>\n",
       "      <td>0.277432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.115462</td>\n",
       "      <td>0.277309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.113449</td>\n",
       "      <td>0.256664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.11165</td>\n",
       "      <td>0.268348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.116635</td>\n",
       "      <td>0.334909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.114395</td>\n",
       "      <td>0.262341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.112693</td>\n",
       "      <td>0.325823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.109387</td>\n",
       "      <td>0.257326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.109874</td>\n",
       "      <td>0.295709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.110436</td>\n",
       "      <td>0.271062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.106653</td>\n",
       "      <td>0.264793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.106946</td>\n",
       "      <td>0.312468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.108092</td>\n",
       "      <td>0.248727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.133078</td>\n",
       "      <td>0.319315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.122107</td>\n",
       "      <td>0.275251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.114691</td>\n",
       "      <td>0.262148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.111273</td>\n",
       "      <td>0.317153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.104818</td>\n",
       "      <td>0.319947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.106758</td>\n",
       "      <td>0.305964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.106236</td>\n",
       "      <td>0.304221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.102493</td>\n",
       "      <td>0.317164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.102772</td>\n",
       "      <td>0.307075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.101632</td>\n",
       "      <td>0.330242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.104492</td>\n",
       "      <td>0.278956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.103787</td>\n",
       "      <td>0.276821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.102495</td>\n",
       "      <td>0.339464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_loss  val_loss\n",
       "0     0.59701  0.268954\n",
       "1    0.377755  0.236426\n",
       "2    0.293683  0.233279\n",
       "3    0.247144  0.217064\n",
       "4    0.211694  0.223629\n",
       "5    0.197305  0.218176\n",
       "6    0.190168  0.227458\n",
       "7    0.184956  0.249997\n",
       "8    0.165634  0.248771\n",
       "9    0.156117  0.228524\n",
       "10   0.142821  0.218557\n",
       "11   0.144597  0.234908\n",
       "12   0.139412  0.226541\n",
       "13   0.137074  0.239189\n",
       "14   0.142013  0.254521\n",
       "15   0.130362  0.279829\n",
       "16   0.128191  0.287619\n",
       "17   0.124188  0.252116\n",
       "18   0.123952   0.25563\n",
       "19   0.122601   0.26618\n",
       "20   0.118053  0.239822\n",
       "21   0.120307  0.295306\n",
       "22   0.122055  0.288052\n",
       "23   0.116096  0.253086\n",
       "24   0.115568  0.277432\n",
       "25   0.115462  0.277309\n",
       "26   0.113449  0.256664\n",
       "27    0.11165  0.268348\n",
       "28   0.116635  0.334909\n",
       "29   0.114395  0.262341\n",
       "30   0.112693  0.325823\n",
       "31   0.109387  0.257326\n",
       "32   0.109874  0.295709\n",
       "33   0.110436  0.271062\n",
       "34   0.106653  0.264793\n",
       "35   0.106946  0.312468\n",
       "36   0.108092  0.248727\n",
       "37   0.133078  0.319315\n",
       "38   0.122107  0.275251\n",
       "39   0.114691  0.262148\n",
       "40   0.111273  0.317153\n",
       "41   0.104818  0.319947\n",
       "42   0.106758  0.305964\n",
       "43   0.106236  0.304221\n",
       "44   0.102493  0.317164\n",
       "45   0.102772  0.307075\n",
       "46   0.101632  0.330242\n",
       "47   0.104492  0.278956\n",
       "48   0.103787  0.276821\n",
       "49   0.102495  0.339464"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_df = pd.DataFrame() \n",
    "#new_df['epoch'] = train_loss_df['epoch'] \n",
    "new_df['train_loss'] = train_loss_df['total_loss'] \n",
    "new_df['val_loss'] = val_loss_df['total_loss'] \n",
    "\n",
    "sns.lineplot(data = new_df[1:])\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49cfaca4-1cd8-4189-bea6-512a2cf7e834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss_classifier</th>\n",
       "      <th>loss_box_reg</th>\n",
       "      <th>loss_mask</th>\n",
       "      <th>loss_objectness</th>\n",
       "      <th>loss_rpn_box_reg</th>\n",
       "      <th>total_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.085762</td>\n",
       "      <td>0.042465</td>\n",
       "      <td>0.365576</td>\n",
       "      <td>0.05592</td>\n",
       "      <td>0.047286</td>\n",
       "      <td>0.59701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.058815</td>\n",
       "      <td>0.053937</td>\n",
       "      <td>0.229459</td>\n",
       "      <td>0.017576</td>\n",
       "      <td>0.017968</td>\n",
       "      <td>0.377755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.042723</td>\n",
       "      <td>0.043495</td>\n",
       "      <td>0.181428</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>0.016068</td>\n",
       "      <td>0.293683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.031149</td>\n",
       "      <td>0.034787</td>\n",
       "      <td>0.159233</td>\n",
       "      <td>0.006336</td>\n",
       "      <td>0.015639</td>\n",
       "      <td>0.247144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.02967</td>\n",
       "      <td>0.144604</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.211694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.019854</td>\n",
       "      <td>0.026669</td>\n",
       "      <td>0.139947</td>\n",
       "      <td>0.00383</td>\n",
       "      <td>0.007005</td>\n",
       "      <td>0.197305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.017394</td>\n",
       "      <td>0.023706</td>\n",
       "      <td>0.140541</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.005554</td>\n",
       "      <td>0.190168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.015154</td>\n",
       "      <td>0.021927</td>\n",
       "      <td>0.139653</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.184956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.012956</td>\n",
       "      <td>0.020134</td>\n",
       "      <td>0.124798</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>0.005719</td>\n",
       "      <td>0.165634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.011497</td>\n",
       "      <td>0.018778</td>\n",
       "      <td>0.117906</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.006279</td>\n",
       "      <td>0.156117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.010294</td>\n",
       "      <td>0.01675</td>\n",
       "      <td>0.110128</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.142821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.009607</td>\n",
       "      <td>0.01609</td>\n",
       "      <td>0.113605</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>0.144597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.008754</td>\n",
       "      <td>0.015184</td>\n",
       "      <td>0.110684</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.139412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.008517</td>\n",
       "      <td>0.014876</td>\n",
       "      <td>0.109252</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>0.137074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.009314</td>\n",
       "      <td>0.014635</td>\n",
       "      <td>0.112678</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>0.142013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.008903</td>\n",
       "      <td>0.013746</td>\n",
       "      <td>0.103258</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.003123</td>\n",
       "      <td>0.130362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>0.013216</td>\n",
       "      <td>0.102515</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.003553</td>\n",
       "      <td>0.128191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.007505</td>\n",
       "      <td>0.012724</td>\n",
       "      <td>0.10047</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.002447</td>\n",
       "      <td>0.124188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.007229</td>\n",
       "      <td>0.012516</td>\n",
       "      <td>0.100509</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>0.123952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.006861</td>\n",
       "      <td>0.01179</td>\n",
       "      <td>0.099454</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.122601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.006406</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0.096476</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.118053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.00656</td>\n",
       "      <td>0.011217</td>\n",
       "      <td>0.097808</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.00378</td>\n",
       "      <td>0.120307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.006582</td>\n",
       "      <td>0.011046</td>\n",
       "      <td>0.10104</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>0.122055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.006177</td>\n",
       "      <td>0.010389</td>\n",
       "      <td>0.096323</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.002161</td>\n",
       "      <td>0.116096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.095989</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.115568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.005904</td>\n",
       "      <td>0.010724</td>\n",
       "      <td>0.095733</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.115462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.005808</td>\n",
       "      <td>0.010179</td>\n",
       "      <td>0.093988</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>0.113449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.005577</td>\n",
       "      <td>0.009718</td>\n",
       "      <td>0.093213</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.11165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.005674</td>\n",
       "      <td>0.009938</td>\n",
       "      <td>0.098116</td>\n",
       "      <td>0.00094</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.116635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.006056</td>\n",
       "      <td>0.010501</td>\n",
       "      <td>0.094405</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.114395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.005482</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.094636</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.112693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.00533</td>\n",
       "      <td>0.008711</td>\n",
       "      <td>0.092212</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.109387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.009073</td>\n",
       "      <td>0.092261</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.109874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.005211</td>\n",
       "      <td>0.008945</td>\n",
       "      <td>0.093173</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.110436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.004932</td>\n",
       "      <td>0.008295</td>\n",
       "      <td>0.090124</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.106653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.00521</td>\n",
       "      <td>0.008826</td>\n",
       "      <td>0.088746</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>0.106946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>0.008128</td>\n",
       "      <td>0.092564</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.00153</td>\n",
       "      <td>0.108092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.006168</td>\n",
       "      <td>0.010336</td>\n",
       "      <td>0.111938</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.133078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0.006322</td>\n",
       "      <td>0.009894</td>\n",
       "      <td>0.102075</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.122107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.008449</td>\n",
       "      <td>0.09682</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.00242</td>\n",
       "      <td>0.114691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>0.008116</td>\n",
       "      <td>0.093194</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.111273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>0.007712</td>\n",
       "      <td>0.089396</td>\n",
       "      <td>0.00093</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>0.104818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.005143</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.09061</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.106758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.004915</td>\n",
       "      <td>0.007712</td>\n",
       "      <td>0.091024</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.106236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.005016</td>\n",
       "      <td>0.007465</td>\n",
       "      <td>0.086982</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.00197</td>\n",
       "      <td>0.102493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.007097</td>\n",
       "      <td>0.08799</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.102772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.007263</td>\n",
       "      <td>0.087041</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.101632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>0.089334</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>0.104492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>0.007858</td>\n",
       "      <td>0.088878</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.103787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.007611</td>\n",
       "      <td>0.087631</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.00152</td>\n",
       "      <td>0.102495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch loss_classifier loss_box_reg loss_mask loss_objectness  \\\n",
       "0      0        0.085762     0.042465  0.365576         0.05592   \n",
       "1      1        0.058815     0.053937  0.229459        0.017576   \n",
       "2      2        0.042723     0.043495  0.181428        0.009969   \n",
       "3      3        0.031149     0.034787  0.159233        0.006336   \n",
       "4      4        0.024061      0.02967  0.144604        0.004676   \n",
       "5      5        0.019854     0.026669  0.139947         0.00383   \n",
       "6      6        0.017394     0.023706  0.140541        0.002972   \n",
       "7      7        0.015154     0.021927  0.139653        0.002521   \n",
       "8      8        0.012956     0.020134  0.124798        0.002027   \n",
       "9      9        0.011497     0.018778  0.117906        0.001658   \n",
       "10    10        0.010294      0.01675  0.110128        0.001564   \n",
       "11    11        0.009607      0.01609  0.113605        0.001486   \n",
       "12    12        0.008754     0.015184  0.110684        0.001399   \n",
       "13    13        0.008517     0.014876  0.109252        0.001337   \n",
       "14    14        0.009314     0.014635  0.112678        0.001418   \n",
       "15    15        0.008903     0.013746  0.103258        0.001332   \n",
       "16    16        0.007449     0.013216  0.102515        0.001457   \n",
       "17    17        0.007505     0.012724   0.10047        0.001043   \n",
       "18    18        0.007229     0.012516  0.100509        0.001186   \n",
       "19    19        0.006861      0.01179  0.099454        0.001269   \n",
       "20    20        0.006406     0.011455  0.096476        0.001085   \n",
       "21    21         0.00656     0.011217  0.097808        0.000942   \n",
       "22    22        0.006582     0.011046   0.10104        0.000941   \n",
       "23    23        0.006177     0.010389  0.096323        0.001045   \n",
       "24    24        0.006053       0.0101  0.095989        0.001143   \n",
       "25    25        0.005904     0.010724  0.095733        0.001082   \n",
       "26    26        0.005808     0.010179  0.093988        0.001139   \n",
       "27    27        0.005577     0.009718  0.093213        0.001045   \n",
       "28    28        0.005674     0.009938  0.098116         0.00094   \n",
       "29    29        0.006056     0.010501  0.094405        0.001159   \n",
       "30    30        0.005482     0.009787  0.094636        0.000974   \n",
       "31    31         0.00533     0.008711  0.092212        0.001036   \n",
       "32    32        0.005217     0.009073  0.092261        0.001057   \n",
       "33    33        0.005211     0.008945  0.093173        0.001011   \n",
       "34    34        0.004932     0.008295  0.090124        0.001219   \n",
       "35    35         0.00521     0.008826  0.088746        0.001134   \n",
       "36    36        0.004993     0.008128  0.092564        0.000877   \n",
       "37    37        0.006168     0.010336  0.111938        0.001353   \n",
       "38    38        0.006322     0.009894  0.102075        0.001406   \n",
       "39    39        0.005704     0.008449   0.09682        0.001299   \n",
       "40    40        0.005653     0.008116  0.093194        0.001121   \n",
       "41    41        0.005023     0.007712  0.089396         0.00093   \n",
       "42    42        0.005143     0.007843   0.09061        0.001138   \n",
       "43    43        0.004915     0.007712  0.091024        0.001048   \n",
       "44    44        0.005016     0.007465  0.086982        0.001059   \n",
       "45    45        0.004831     0.007097   0.08799        0.000951   \n",
       "46    46        0.004713     0.007263  0.087041        0.000998   \n",
       "47    47        0.004967     0.007419  0.089334        0.000951   \n",
       "48    48        0.004702     0.007858  0.088878        0.000901   \n",
       "49    49        0.004713     0.007611  0.087631        0.001021   \n",
       "\n",
       "   loss_rpn_box_reg total_loss  \n",
       "0          0.047286    0.59701  \n",
       "1          0.017968   0.377755  \n",
       "2          0.016068   0.293683  \n",
       "3          0.015639   0.247144  \n",
       "4          0.008682   0.211694  \n",
       "5          0.007005   0.197305  \n",
       "6          0.005554   0.190168  \n",
       "7            0.0057   0.184956  \n",
       "8          0.005719   0.165634  \n",
       "9          0.006279   0.156117  \n",
       "10         0.004086   0.142821  \n",
       "11         0.003809   0.144597  \n",
       "12          0.00339   0.139412  \n",
       "13         0.003092   0.137074  \n",
       "14         0.003969   0.142013  \n",
       "15         0.003123   0.130362  \n",
       "16         0.003553   0.128191  \n",
       "17         0.002447   0.124188  \n",
       "18         0.002511   0.123952  \n",
       "19         0.003227   0.122601  \n",
       "20         0.002631   0.118053  \n",
       "21          0.00378   0.120307  \n",
       "22         0.002446   0.122055  \n",
       "23         0.002161   0.116096  \n",
       "24         0.002284   0.115568  \n",
       "25         0.002018   0.115462  \n",
       "26         0.002335   0.113449  \n",
       "27         0.002096    0.11165  \n",
       "28         0.001967   0.116635  \n",
       "29         0.002275   0.114395  \n",
       "30         0.001815   0.112693  \n",
       "31         0.002099   0.109387  \n",
       "32         0.002266   0.109874  \n",
       "33         0.002097   0.110436  \n",
       "34         0.002084   0.106653  \n",
       "35         0.003029   0.106946  \n",
       "36          0.00153   0.108092  \n",
       "37         0.003283   0.133078  \n",
       "38          0.00241   0.122107  \n",
       "39          0.00242   0.114691  \n",
       "40         0.003189   0.111273  \n",
       "41         0.001758   0.104818  \n",
       "42         0.002023   0.106758  \n",
       "43         0.001537   0.106236  \n",
       "44          0.00197   0.102493  \n",
       "45         0.001902   0.102772  \n",
       "46         0.001617   0.101632  \n",
       "47         0.001821   0.104492  \n",
       "48         0.001447   0.103787  \n",
       "49          0.00152   0.102495  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6116f24-8ab3-4546-9126-a23d4e476566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss_classifier</th>\n",
       "      <th>loss_box_reg</th>\n",
       "      <th>loss_mask</th>\n",
       "      <th>loss_objectness</th>\n",
       "      <th>loss_rpn_box_reg</th>\n",
       "      <th>total_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.132092</td>\n",
       "      <td>0.117159</td>\n",
       "      <td>0.283429</td>\n",
       "      <td>0.03464</td>\n",
       "      <td>0.026286</td>\n",
       "      <td>0.593607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116492</td>\n",
       "      <td>0.136724</td>\n",
       "      <td>0.247314</td>\n",
       "      <td>0.030745</td>\n",
       "      <td>0.016726</td>\n",
       "      <td>0.548002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  epoch loss_classifier loss_box_reg loss_mask loss_objectness  \\\n",
       "0     0        0.132092     0.117159  0.283429         0.03464   \n",
       "1     1        0.116492     0.136724  0.247314        0.030745   \n",
       "\n",
       "  loss_rpn_box_reg total_loss  \n",
       "0         0.026286   0.593607  \n",
       "1         0.016726   0.548002  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf2fbde-bff3-4e6b-9d93-e3d2cb66a372",
   "metadata": {},
   "source": [
    "# Calculate mAP on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01df9b0b-83a4-420c-b5fd-65867289263e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.metrics' from '/home/INM705/INM705_CW_Collins_Velagala/notebooks/../src/metrics.py'>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4eec9a48-789e-4f2c-a848-760f345c3f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    " val_data, batch_size=1, shuffle=True, #num_workers=4,\n",
    " collate_fn=helper.CollateCustom())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fbbc1905-8ea8-4a86-9762-360372e7deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_boxes =[]\n",
    "gt =[] \n",
    "\n",
    "for idx,X, y in test_loader:\n",
    "    model.eval()\n",
    "    y_pred = model(X.to(device)) \n",
    "    pred_boxes, gt = metrics.store_preds(idx, y, y_pred, pred_boxes, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b65e857-cbaf-40b1-95db-6d9b27176dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2103\n",
      "class 1: len detections: 307\n",
      "for category 1:\n",
      "-------\n",
      "AP is 0.36108115315437317\n",
      "-------\n",
      "\n",
      "class 2: len detections: 236\n",
      "for category 2:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 3: len detections: 389\n",
      "for category 3:\n",
      "-------\n",
      "AP is 0.703338086605072\n",
      "-------\n",
      "\n",
      "class 4: len detections: 575\n",
      "for category 4:\n",
      "-------\n",
      "AP is 0.2933984398841858\n",
      "-------\n",
      "\n",
      "class 5: len detections: 241\n",
      "for category 5:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 6: len detections: 355\n",
      "for category 6:\n",
      "-------\n",
      "AP is 0.08467690646648407\n",
      "-------\n",
      "\n",
      "2103\n",
      "class 1: len detections: 307\n",
      "for category 1:\n",
      "-------\n",
      "AP is 0.36108115315437317\n",
      "-------\n",
      "\n",
      "class 2: len detections: 236\n",
      "for category 2:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 3: len detections: 389\n",
      "for category 3:\n",
      "-------\n",
      "AP is 0.703338086605072\n",
      "-------\n",
      "\n",
      "class 4: len detections: 575\n",
      "for category 4:\n",
      "-------\n",
      "AP is 0.2933984398841858\n",
      "-------\n",
      "\n",
      "class 5: len detections: 241\n",
      "for category 5:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 6: len detections: 355\n",
      "for category 6:\n",
      "-------\n",
      "AP is 0.08467690646648407\n",
      "-------\n",
      "\n",
      "2103\n",
      "class 1: len detections: 307\n",
      "for category 1:\n",
      "-------\n",
      "AP is 0.36108115315437317\n",
      "-------\n",
      "\n",
      "class 2: len detections: 236\n",
      "for category 2:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 3: len detections: 389\n",
      "for category 3:\n",
      "-------\n",
      "AP is 0.703338086605072\n",
      "-------\n",
      "\n",
      "class 4: len detections: 575\n",
      "for category 4:\n",
      "-------\n",
      "AP is 0.2933984398841858\n",
      "-------\n",
      "\n",
      "class 5: len detections: 241\n",
      "for category 5:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 6: len detections: 355\n",
      "for category 6:\n",
      "-------\n",
      "AP is 0.08467690646648407\n",
      "-------\n",
      "\n",
      "2103\n",
      "class 1: len detections: 307\n",
      "for category 1:\n",
      "-------\n",
      "AP is 0.35895612835884094\n",
      "-------\n",
      "\n",
      "class 2: len detections: 236\n",
      "for category 2:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 3: len detections: 389\n",
      "for category 3:\n",
      "-------\n",
      "AP is 0.703338086605072\n",
      "-------\n",
      "\n",
      "class 4: len detections: 575\n",
      "for category 4:\n",
      "-------\n",
      "AP is 0.28503939509391785\n",
      "-------\n",
      "\n",
      "class 5: len detections: 241\n",
      "for category 5:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 6: len detections: 355\n",
      "for category 6:\n",
      "-------\n",
      "AP is 0.08467690646648407\n",
      "-------\n",
      "\n",
      "2103\n",
      "class 1: len detections: 307\n",
      "for category 1:\n",
      "-------\n",
      "AP is 0.2560553550720215\n",
      "-------\n",
      "\n",
      "class 2: len detections: 236\n",
      "for category 2:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 3: len detections: 389\n",
      "for category 3:\n",
      "-------\n",
      "AP is 0.6303144097328186\n",
      "-------\n",
      "\n",
      "class 4: len detections: 575\n",
      "for category 4:\n",
      "-------\n",
      "AP is 0.2755574584007263\n",
      "-------\n",
      "\n",
      "class 5: len detections: 241\n",
      "for category 5:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 6: len detections: 355\n",
      "for category 6:\n",
      "-------\n",
      "AP is 0.023741941899061203\n",
      "-------\n",
      "\n",
      "2103\n",
      "class 1: len detections: 307\n",
      "for category 1:\n",
      "-------\n",
      "AP is 0.21698874235153198\n",
      "-------\n",
      "\n",
      "class 2: len detections: 236\n",
      "for category 2:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 3: len detections: 389\n",
      "for category 3:\n",
      "-------\n",
      "AP is 0.5530869960784912\n",
      "-------\n",
      "\n",
      "class 4: len detections: 575\n",
      "for category 4:\n",
      "-------\n",
      "AP is 0.2106320708990097\n",
      "-------\n",
      "\n",
      "class 5: len detections: 241\n",
      "for category 5:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 6: len detections: 355\n",
      "for category 6:\n",
      "-------\n",
      "AP is 0.023741941899061203\n",
      "-------\n",
      "\n",
      "2103\n",
      "class 1: len detections: 307\n",
      "for category 1:\n",
      "-------\n",
      "AP is 0.21562832593917847\n",
      "-------\n",
      "\n",
      "class 2: len detections: 236\n",
      "for category 2:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 3: len detections: 389\n",
      "for category 3:\n",
      "-------\n",
      "AP is 0.44435960054397583\n",
      "-------\n",
      "\n",
      "class 4: len detections: 575\n",
      "for category 4:\n",
      "-------\n",
      "AP is 0.11772395670413971\n",
      "-------\n",
      "\n",
      "class 5: len detections: 241\n",
      "for category 5:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 6: len detections: 355\n",
      "for category 6:\n",
      "-------\n",
      "AP is 0.023741941899061203\n",
      "-------\n",
      "\n",
      "2103\n",
      "class 1: len detections: 307\n",
      "for category 1:\n",
      "-------\n",
      "AP is 0.11742040514945984\n",
      "-------\n",
      "\n",
      "class 2: len detections: 236\n",
      "for category 2:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 3: len detections: 389\n",
      "for category 3:\n",
      "-------\n",
      "AP is 0.3164007365703583\n",
      "-------\n",
      "\n",
      "class 4: len detections: 575\n",
      "for category 4:\n",
      "-------\n",
      "AP is 0.05410905182361603\n",
      "-------\n",
      "\n",
      "class 5: len detections: 241\n",
      "for category 5:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 6: len detections: 355\n",
      "for category 6:\n",
      "-------\n",
      "AP is 0.0009057970601134002\n",
      "-------\n",
      "\n",
      "2103\n",
      "class 1: len detections: 307\n",
      "for category 1:\n",
      "-------\n",
      "AP is 0.1104307770729065\n",
      "-------\n",
      "\n",
      "class 2: len detections: 236\n",
      "for category 2:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 3: len detections: 389\n",
      "for category 3:\n",
      "-------\n",
      "AP is 0.0886755883693695\n",
      "-------\n",
      "\n",
      "class 4: len detections: 575\n",
      "for category 4:\n",
      "-------\n",
      "AP is 0.01478363573551178\n",
      "-------\n",
      "\n",
      "class 5: len detections: 241\n",
      "for category 5:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 6: len detections: 355\n",
      "for category 6:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "2103\n",
      "class 1: len detections: 307\n",
      "for category 1:\n",
      "-------\n",
      "AP is 0.03056454099714756\n",
      "-------\n",
      "\n",
      "class 2: len detections: 236\n",
      "for category 2:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 3: len detections: 389\n",
      "for category 3:\n",
      "-------\n",
      "AP is 0.010367260314524174\n",
      "-------\n",
      "\n",
      "class 4: len detections: 575\n",
      "for category 4:\n",
      "-------\n",
      "AP is 0.012658222578465939\n",
      "-------\n",
      "\n",
      "class 5: len detections: 241\n",
      "for category 5:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "class 6: len detections: 355\n",
      "for category 6:\n",
      "-------\n",
      "AP is 0.0\n",
      "-------\n",
      "\n",
      "mAP for iout_threshold of 0.5: 0.2404157519340515\n",
      "mAP for iout_threshold of 0.55: 0.2404157519340515\n",
      "mAP for iout_threshold of 0.6000000000000001: 0.2404157519340515\n",
      "mAP for iout_threshold of 0.6500000000000001: 0.23866842687129974\n",
      "mAP for iout_threshold of 0.7000000000000002: 0.19761152565479279\n",
      "mAP for iout_threshold of 0.7500000000000002: 0.167408287525177\n",
      "mAP for iout_threshold of 0.8000000000000003: 0.13357563316822052\n",
      "mAP for iout_threshold of 0.8500000000000003: 0.08147266507148743\n",
      "mAP for iout_threshold of 0.9000000000000004: 0.03564833477139473\n",
      "mAP for iout_threshold of 0.9500000000000004: 0.008931671269237995\n"
     ]
    }
   ],
   "source": [
    "mAP_list = [] \n",
    "thresholds= np.arange(0.5, 1, 0.05) #0.5 -0.95\n",
    "for iou_thres in thresholds:\n",
    "    mAP_list.append(metrics.calculate_ap(pred_boxes, gt, iou_threshold = iou_thres,\n",
    "                                         num_classes = len(val_data.class_idx_map)))\n",
    "\n",
    "for i in range(len(thresholds)):\n",
    "        print(f'mAP for iout_threshold of {thresholds[i]}: {mAP_list[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0bd63-b437-4ad3-9be0-df0cbad893d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
