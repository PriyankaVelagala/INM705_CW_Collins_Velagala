{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df43f32-7949-4e7a-90be-f6797c667b83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting ujson\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/f8/8c/5274ba7b4df814c87a8840a58e2b1dae6a489f49c3b0fad2d15f1e41d47b/ujson-5.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45 kB)\n",
      "Installing collected packages: ujson\n",
      "Successfully installed ujson-5.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting seaborn\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/10/5b/0479d7d845b5ba410ca702ffcd7f2cd95a14a4dfff1fde2637802b258b9b/seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.6.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.20.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (4.33.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (9.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (20.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.11.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ujson \n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8110a508-5c22-443e-8c2d-1f2785961a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc16b26c-4d09-4f3e-9f19-dcb81493fcd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "import sys \n",
    "import json \n",
    "from src import dataset_lvis\n",
    "from src import metrics\n",
    "from src import helper_functions as helper\n",
    "import importlib\n",
    "from pathlib import Path \n",
    "from PIL import Image as PILImage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pandas as pd \n",
    "import time \n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "713acfd1-c1cf-44fa-b17f-bb640870fab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.dataset_lvis' from '/home/INM705_CW_Collins_Velagala/notebooks/../src/dataset_lvis.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dataset_lvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97408296-2f70-4eaa-8243-c5fdb4345c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fa061-629f-46cb-b3df-99a9c7afbcde",
   "metadata": {},
   "source": [
    "# Load train and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39685c2e-8366-47b5-9e5b-6b40e2d8c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes : {1: 'apricot', 2: 'crouton', 3: 'crumb', 4: 'grape', 5: 'pea_(food)', 6: 'peach'}\n",
      "loaded 628 positive set images\n",
      "loaded 0 negative set images\n",
      "loaded 341 non-exhaustive set images\n",
      "Loaded 287 images!\n",
      "class 1 has 10 positive and 0 negative images\n",
      "class 2 has 7 positive and 0 negative images\n",
      "class 3 has 101 positive and 0 negative images\n",
      "class 4 has 101 positive and 0 negative images\n",
      "class 5 has 31 positive and 0 negative images\n",
      "class 6 has 47 positive and 0 negative images\n",
      "3283 annotations found!\n",
      "stage:  train\n",
      "classes:  {'apricot': 14, 'crouton': 331, 'crumb': 338, 'grape': 510, 'pea_(food)': 773, 'peach': 774}\n",
      "ds_path:  ../Datasets/coco/\n",
      "labels_f:  ../Datasets/coco/annotations/lvis_v1_train.json\n",
      "imgs_dir:  ../Datasets/coco/images/train2017\n",
      "Time taken to initialize train set: 42.40775275230408\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "data_args = {'stage': 'train',\n",
    "            'classes': ['grape', 'crumb', 'pea_(food)',\n",
    "                        'peach', 'apricot', 'crouton'], \n",
    "            'ds_path' : \"../Datasets/coco/\",\n",
    "            'labels_dir': \"annotations\",\n",
    "            'images_dir': 'images',\n",
    "             'height' : 480,\n",
    "             'width' : 640,\n",
    "            'max_negative' : 0}\n",
    "train_data = dataset_lvis.LVISData(**data_args)\n",
    "\n",
    "print(f'Time taken to initialize train set: {time.time()-time_start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965951b-bfb1-48e8-8deb-c320b8d98b2f",
   "metadata": {},
   "source": [
    "### Split train set into custom train val set\n",
    "\n",
    "Validation set reserved for testing\n",
    "\n",
    "Check that we don't have too many of the rare cases in validation set - if so then there won't be much data to actually train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec426e15-a300-45cf-8d66-ee6dc4ffdd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0, 2: 0, 3: 55, 4: 198, 5: 79, 6: 27, 7: 0, 8: 0, 9: 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what images have what classes?\n",
    "g_cpu = torch.Generator()\n",
    "g_cpu.manual_seed(42)\n",
    "\n",
    "#Split dataset to test and train\n",
    "indices = torch.randperm(len(train_data), generator=g_cpu).tolist()\n",
    "\n",
    "\n",
    "dataset_train = torch.utils.data.Subset(train_data, indices[:260])\n",
    "dataset_val = torch.utils.data.Subset(train_data, indices[260:])\n",
    "\n",
    "\n",
    "class_counts = dict(zip(range(1,10), [0]*9))\n",
    "for _, _, y in dataset_val:\n",
    "    for label in y['labels']:\n",
    "        class_counts[label.item()] += 1\n",
    "            \n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed89f8-a249-4680-bc8a-e39015c2c63d",
   "metadata": {},
   "source": [
    "## Prepare test set\n",
    "\n",
    "The LVIS validation set is reserved for our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "754ea837-6704-449b-a0b3-c5768d1b453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes : {1: 'apricot', 2: 'crouton', 3: 'crumb', 4: 'grape', 5: 'pea_(food)', 6: 'peach'}\n",
      "loaded 117 positive set images\n",
      "loaded 887 negative set images\n",
      "loaded 59 non-exhaustive set images\n",
      "Loaded 942 images!\n",
      "class 1 has 0 positive and 119 negative images\n",
      "class 2 has 3 positive and 140 negative images\n",
      "class 3 has 18 positive and 124 negative images\n",
      "class 4 has 22 positive and 174 negative images\n",
      "class 5 has 7 positive and 181 negative images\n",
      "class 6 has 9 positive and 169 negative images\n",
      "791 annotations found!\n",
      "stage:  val\n",
      "classes:  {'apricot': 14, 'crouton': 331, 'crumb': 338, 'grape': 510, 'pea_(food)': 773, 'peach': 774}\n",
      "ds_path:  ../Datasets/coco/\n",
      "labels_f:  ../Datasets/coco/annotations/lvis_v1_val.json\n",
      "imgs_dir:  ../Datasets/coco/images/train2017\n",
      "Time taken to initialize val set: 4.068005561828613\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time() \n",
    "data_args = {'stage': 'val',\n",
    "            'classes': ['grape', 'crumb', 'pea_(food)',\n",
    "                        'peach', 'apricot', 'crouton'], \n",
    "            'ds_path' : \"../Datasets/coco/\",\n",
    "            'labels_dir': \"annotations\",\n",
    "            'images_dir': 'images',\n",
    "             'height' : 480,\n",
    "             'width' : 640,\n",
    "            'max_negative' : 200}   # note that we include negative sets for the testing\n",
    "test_data = dataset_lvis.LVISData(**data_args)\n",
    "print(f'Time taken to initialize val set: {time.time()-time_start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5210fb-ba67-46c9-a72d-2f93d89e672f",
   "metadata": {},
   "source": [
    "# Fine-tuning the model\n",
    "\n",
    "Here we set `pretrained = False` for the FPN: FPN is pre-trained on coco dataset so we'll try to retrain from scratch. We keep the backbone ResNet50 pretrained on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc402d35-ad82-4540-a29a-ed74706f7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41364461-6867-4f49-aa53-177da9f90eb2",
   "metadata": {},
   "source": [
    "# Set up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ff5ce4e-729b-49b4-a1e0-28d48fc9049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    " dataset_train, batch_size=4, shuffle=True, #num_workers=4,\n",
    " collate_fn=helper.CollateCustom())\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    " dataset_val, batch_size=4, shuffle=True, #num_workers=4,\n",
    " collate_fn=helper.CollateCustom())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af324036-b825-4573-a784-343b9dbb3a47",
   "metadata": {},
   "source": [
    "# Initialize Model + Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d139c611-3d48-4c57-8147-9fa50f2bee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has 10 classes - background and 9 specified objects\n",
    "num_classes = 10\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d532151-671e-4846-aa83-34678660c4b7",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034c2c8-91f2-4546-84ea-b3f0085c75bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 0 epochs: 92.93645197153091\n",
      "Validation loss after 0 epochs: 8.893099248409271\n",
      "Time elapsed for 0 epochs: 47.035053730010986\n",
      "Saved checkpoint model_0_epochs.pth!\n",
      "Time elapsed after 0 epochs: 47.53584337234497\n",
      "Training loss after 1 epochs: 70.51079231500626\n",
      "Validation loss after 1 epochs: 8.07829761505127\n",
      "Time elapsed for 1 epochs: 95.66369438171387\n",
      "Training loss after 2 epochs: 61.59013074636459\n",
      "Validation loss after 2 epochs: 8.422475576400757\n",
      "Time elapsed for 2 epochs: 143.78244352340698\n",
      "Training loss after 3 epochs: 55.91294676065445\n",
      "Validation loss after 3 epochs: 9.219867527484894\n",
      "Time elapsed for 3 epochs: 192.26619291305542\n",
      "Training loss after 4 epochs: 52.95373582839966\n",
      "Validation loss after 4 epochs: 8.494624316692352\n",
      "Time elapsed for 4 epochs: 240.8719892501831\n",
      "Training loss after 5 epochs: 50.05961659550667\n",
      "Validation loss after 5 epochs: 8.63841313123703\n",
      "Time elapsed for 5 epochs: 289.78059673309326\n",
      "Saved checkpoint model_5_epochs.pth!\n",
      "Time elapsed after 5 epochs: 290.28244256973267\n",
      "Training loss after 6 epochs: 45.32026016712189\n",
      "Validation loss after 6 epochs: 9.478495359420776\n",
      "Time elapsed for 6 epochs: 339.45872497558594\n",
      "Training loss after 7 epochs: 44.035542130470276\n",
      "Validation loss after 7 epochs: 10.434440016746521\n",
      "Time elapsed for 7 epochs: 388.7474105358124\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 31\n",
    "time_start = time.time() \n",
    "\n",
    "\n",
    "train_loss_df = pd.DataFrame(columns = ['epoch', 'loss_classifier', 'loss_box_reg', 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg', 'total_loss'])\n",
    "val_loss_df = pd.DataFrame(columns = ['epoch', 'loss_classifier', 'loss_box_reg', 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg', 'total_loss'])\n",
    "\n",
    "loss_types = ['loss_classifier', 'loss_box_reg', 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg', 'total_loss']\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = dict.fromkeys(loss_types, 0) \n",
    "    val_loss = dict.fromkeys(loss_types, 0) \n",
    "    \n",
    "    \"\"\"\n",
    "    Train \n",
    "    \"\"\"\n",
    "    for batch_num, (idx, X, y) in enumerate(train_loader):\n",
    "        #print(idx)\n",
    "        X = X.to(device)\n",
    "        y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
    "        \n",
    "        loss_dict = model(X, y) \n",
    "        \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        #save losses\n",
    "        for key in loss_types:\n",
    "            if key != 'total_loss':\n",
    "                train_loss[key] += loss_dict[key].item()\n",
    "            else: \n",
    "                train_loss['total_loss'] += losses.item()\n",
    "                \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    row = pd.DataFrame({'epoch': [epoch],\n",
    "          'loss_classifier': [train_loss['loss_classifier']/(batch_num+1)],\n",
    "          'loss_box_reg' : [train_loss['loss_box_reg']/(batch_num+1)],\n",
    "           'loss_mask': [train_loss['loss_mask']/(batch_num+1)],\n",
    "           'loss_objectness': [train_loss['loss_objectness']/(batch_num+1)],\n",
    "           'loss_rpn_box_reg': [train_loss['loss_rpn_box_reg']/(batch_num+1)],\n",
    "            'total_loss': [train_loss['total_loss']/(batch_num+1)] \n",
    "          })     \n",
    "\n",
    "    train_loss_df = pd.concat([train_loss_df, row], ignore_index = True, axis = 0)\n",
    "    \n",
    "    print(f\"Training loss after {epoch} epochs: {train_loss['total_loss']}\")\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Validation\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (idx, X, y) in enumerate(val_loader):\n",
    "            X = X.to(device)\n",
    "            y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
    "\n",
    "            loss_dict = model(X, y) \n",
    "            \n",
    "            losses_val = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            #save losses\n",
    "            for key in loss_types:\n",
    "                if key != 'total_loss':\n",
    "                    val_loss[key] += loss_dict[key].item()\n",
    "                else: \n",
    "                    val_loss['total_loss'] += losses_val.item()\n",
    "                    \n",
    "        row = pd.DataFrame({'epoch': [epoch],\n",
    "                          'loss_classifier': [val_loss['loss_classifier']/(batch_num+1)],\n",
    "                          'loss_box_reg' : [val_loss['loss_box_reg']/(batch_num+1)],\n",
    "                           'loss_mask': [val_loss['loss_mask']/(batch_num+1)],\n",
    "                           'loss_objectness': [val_loss['loss_objectness']/(batch_num+1)],\n",
    "                           'loss_rpn_box_reg': [val_loss['loss_rpn_box_reg']/(batch_num+1)],\n",
    "                            'total_loss': [val_loss['total_loss']/(batch_num+1)] \n",
    "                          })\n",
    "        val_loss_df = pd.concat([val_loss_df, row], ignore_index = True, axis = 0)\n",
    "\n",
    "    print(f\"Validation loss after {epoch} epochs: {val_loss['total_loss']}\") \n",
    "    print(f'Time elapsed for {epoch} epochs: {time.time()-time_start}') \n",
    "\n",
    "      \n",
    "    \"\"\"\n",
    "    Save checkpoints and losses every 5 epoch\n",
    "    \"\"\"\n",
    "    if epoch%5 == 0: \n",
    "        checkpoint = {\"state_dict\": model.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"step\": epoch,\n",
    "                        \"ds_args\" : data_args\n",
    "                        }\n",
    "        fname = \"model_\" + str(epoch) + \"_epochs.pth\"\n",
    "        helper.save_checkpoint(checkpoint, fname)\n",
    "        print(f'Time elapsed after {epoch} epochs: {time.time()-time_start}')  \n",
    "        val_loss_df.to_csv(Path.cwd().parent.joinpath(\"val_loss.csv\"))\n",
    "        train_loss_df.to_csv(Path.cwd().parent.joinpath(\"train_loss.csv\"))\n",
    "\n",
    "    \n",
    "    \n",
    "    #for final epoch \n",
    "    if epoch == num_epochs-1: \n",
    "        helper.save_model(model.state_dict(), \"model.pth\")\n",
    "        val_loss_df.to_csv(Path.cwd().parent.joinpath(\"val_loss.csv\"))\n",
    "        train_loss_df.to_csv(Path.cwd().parent.joinpath(\"train_loss.csv\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "print(f\"Time elapsed for {epoch+1} epochs: {round((time.time()-time_start)/60, 2)} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a9cfa-912a-4885-962c-7f7dc807ccde",
   "metadata": {},
   "source": [
    "# Plot loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f98a05-d7e5-422a-a34e-6f2a9dece363",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame() \n",
    "#new_df['epoch'] = train_loss_df['epoch'] \n",
    "new_df['train_loss'] = train_loss_df['total_loss'] \n",
    "new_df['val_loss'] = val_loss_df['total_loss'] \n",
    "\n",
    "sns.lineplot(data = new_df[1:])\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cfaca4-1cd8-4189-bea6-512a2cf7e834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6116f24-8ab3-4546-9126-a23d4e476566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e4a1c-eeec-454d-a4e1-e309680683d2",
   "metadata": {},
   "source": [
    "## Run the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "618c3906-f0c3-4bcf-88ba-6f4bdff67749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.dataset_lvis' from '/home/INM705_CW_Collins_Velagala/notebooks/../src/dataset_lvis.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(metrics)\n",
    "importlib.reload(dataset_lvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abe51460-a7b3-495c-9832-e7c1be7fec8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    " test_data, batch_size=1, shuffle=True, #num_workers=4,\n",
    " collate_fn=helper.CollateCustom())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e492a-4b1f-483c-bdba-04d5557f4eca",
   "metadata": {},
   "source": [
    "If testing at different time from training, then load checkpoints.\n",
    "\n",
    "Or load checkpoints from earlier stopping point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50934e2d-0ee8-4b7c-aa8a-2a49117941d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint ../model_checkpoints/model_30_epochs.pth!\n"
     ]
    }
   ],
   "source": [
    "cp_path = Path('../model_checkpoints/model_30_epochs.pth')\n",
    "helper.load_checkpoint(cp_path, model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c5b251e-30e4-43eb-b80e-24d4b132627e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1153.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP for iou_threshold of 0.5: 0.185\n",
      "mAP for iou_threshold of 0.55: 0.178\n",
      "mAP for iou_threshold of 0.6: 0.170\n",
      "mAP for iou_threshold of 0.65: 0.157\n",
      "mAP for iou_threshold of 0.7: 0.134\n",
      "mAP for iou_threshold of 0.75: 0.112\n",
      "mAP for iou_threshold of 0.8: 0.079\n",
      "mAP for iou_threshold of 0.85: 0.041\n",
      "mAP for iou_threshold of 0.9: 0.013\n",
      "mAP for iou_threshold of 0.95: 0.000\n",
      "CPU times: user 9min 40s, sys: 17.9 s, total: 9min 58s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pred_boxes = []\n",
    "gt = []\n",
    "for idx, X, y in test_loader:\n",
    "    model.eval()\n",
    "    y_pred = model(X.to(device))\n",
    "    # keep running list of predictions and ground truths\n",
    "    pred_boxes, gt = metrics.store_preds(idx, y, y_pred,\n",
    "                                     pred_boxes, gt)\n",
    "\n",
    "mAP_list = []\n",
    "thresholds = list(np.around(np.arange(0.5, 1.0, 0.05),2))\n",
    "for iou_thresh in thresholds:\n",
    "    mAP_list.append(metrics.calculate_ap(pred_boxes, gt, \n",
    "                               iou_threshold=iou_thresh,\n",
    "                               class_datasets = test_data.class_datasets))\n",
    "\n",
    "for i in range(len(thresholds)):\n",
    "    print('mAP for iou_threshold of {}: {:.3f}'.format(thresholds[i], mAP_list[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a3124-be12-4a3f-bcb5-4c0f23a0f93b",
   "metadata": {},
   "source": [
    "### Print out per category mAP\n",
    "\n",
    "Table below shows the categories we have in rows, and different IoU thresholds in columns.\n",
    "\n",
    "We also print the number of images in the test set containing each category. You can see that the model doesn't have much chance to prove itself on chessboard or drumstick categories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a37a1e9d-16d8-4f6a-ba6e-877cca106e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positives</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.55</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.75</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.85</th>\n",
       "      <th>0.9</th>\n",
       "      <th>0.95</th>\n",
       "      <th>class_mAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crouton</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crumb</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.1545</td>\n",
       "      <td>0.1481</td>\n",
       "      <td>0.1203</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.0697</td>\n",
       "      <td>0.0493</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grape</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.3950</td>\n",
       "      <td>0.3819</td>\n",
       "      <td>0.3721</td>\n",
       "      <td>0.3514</td>\n",
       "      <td>0.3306</td>\n",
       "      <td>0.2867</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.1399</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.2542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pea_(food)</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.3656</td>\n",
       "      <td>0.3407</td>\n",
       "      <td>0.3304</td>\n",
       "      <td>0.3088</td>\n",
       "      <td>0.2403</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>0.0865</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peach</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.1848</td>\n",
       "      <td>0.1848</td>\n",
       "      <td>0.1848</td>\n",
       "      <td>0.1814</td>\n",
       "      <td>0.1529</td>\n",
       "      <td>0.1483</td>\n",
       "      <td>0.1309</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.1264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequent_mean</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.2748</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.2193</td>\n",
       "      <td>0.2002</td>\n",
       "      <td>0.1680</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.0703</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>common_mean</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.2752</td>\n",
       "      <td>0.2627</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.2451</td>\n",
       "      <td>0.1966</td>\n",
       "      <td>0.1631</td>\n",
       "      <td>0.1087</td>\n",
       "      <td>0.0496</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.1575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rare_mean</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               positives     0.5    0.55     0.6    0.65     0.7    0.75  \\\n",
       "crouton              3.0  0.0103  0.0103  0.0103  0.0103  0.0103  0.0103   \n",
       "crumb               18.0  0.1545  0.1481  0.1203  0.0873  0.0697  0.0493   \n",
       "grape               22.0  0.3950  0.3819  0.3721  0.3514  0.3306  0.2867   \n",
       "pea_(food)           7.0  0.3656  0.3407  0.3304  0.3088  0.2403  0.1779   \n",
       "peach                9.0  0.1848  0.1848  0.1848  0.1814  0.1529  0.1483   \n",
       "frequent_mean       20.0  0.2748  0.2650  0.2462  0.2193  0.2002  0.1680   \n",
       "common_mean          8.0  0.2752  0.2627  0.2576  0.2451  0.1966  0.1631   \n",
       "rare_mean            3.0  0.0103  0.0103  0.0103  0.0103  0.0103  0.0103   \n",
       "\n",
       "                  0.8    0.85     0.9    0.95  class_mAP  \n",
       "crouton        0.0066  0.0066  0.0000  0.0000     0.0075  \n",
       "crumb          0.0157  0.0006  0.0001  0.0000     0.0646  \n",
       "grape          0.2354  0.1399  0.0462  0.0023     0.2542  \n",
       "pea_(food)     0.0865  0.0332  0.0020  0.0000     0.1885  \n",
       "peach          0.1309  0.0659  0.0301  0.0003     0.1264  \n",
       "frequent_mean  0.1256  0.0703  0.0232  0.0011     0.1594  \n",
       "common_mean    0.1087  0.0496  0.0160  0.0002     0.1575  \n",
       "rare_mean      0.0066  0.0066  0.0000  0.0000     0.0075  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(index = list(range(1, 7)), columns = ['positives'] + thresholds)\n",
    "\n",
    "df['positives'] = [len(c['positive']) for c in test_data.class_datasets.values()]\n",
    "\n",
    "for i in range(len(thresholds)):\n",
    "    data = [v.item() for v in mAP_list[i][1].values()]\n",
    "    df[thresholds[i]] = data\n",
    "\n",
    "df.rename(index=test_data.idx_to_classname, inplace=True)\n",
    "# remove apricot as no data\n",
    "df = df.iloc[1:,:]\n",
    "df.loc['frequent_mean'] = df[df.positives > 15].mean()\n",
    "df.loc['common_mean'] = df[(df.positives < 15) & (df.positives > 5)].mean()\n",
    "df.loc['rare_mean'] = df[df.positives < 5].mean()\n",
    "df['class_mAP'] = df.iloc[:, 1:].mean(axis=1)\n",
    "df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c53d6-a6df-4a2d-96ae-26e63edf2fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
